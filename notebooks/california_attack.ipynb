{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import sys\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "import transformer_lens.utils as tl_utils\n",
    "from jaxtyping import Float\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tuned_lens.nn import LogitLens, TunedLens\n",
    "from tuned_lens.plotting import PredictionTrajectory\n",
    "\n",
    "from pii import utils\n",
    "from pii.prompt_data import LensData, PromptData\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model / lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "logit_lens = LogitLens.from_model(hf_model).to(\"cuda\")\n",
    "tuned_lens = TunedLens.from_model_and_pretrained(hf_model).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    hf_model=hf_model,\n",
    "    device=\"cuda\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "tl_model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_prompt(question: str, correct: bool = True):\n",
    "    return f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are an assistant who only responds with a single word. \\\n",
    "You only give {'correct' if correct else 'incorrect'} answers.\n",
    "<</SYS>>\n",
    "Complete the statment. {question} [/INST] \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_dontsay_prompt(question: str, forbidden_word: str):\n",
    "    return f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are an obedient assistant who only responds with a single word. \\\n",
    "You will never say the forbidden word \"{forbidden_word}\".\n",
    "<</SYS>>\n",
    "\n",
    "{question} [/INST] \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "question = \"Lionel Messi plays the sport of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, corr in enumerate([True, False]):\n",
    "    prompt = get_corr_prompt(question, correct=corr)\n",
    "    print(prompt)\n",
    "    utils.get_top_responses(\n",
    "        prompt=prompt,\n",
    "        model=tl_model,\n",
    "        top_k=5,\n",
    "    )\n",
    "    if i == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, forbidden_word in enumerate([\"apple\", \"football\"]):\n",
    "    prompt = get_dontsay_prompt(question, forbidden_word)\n",
    "    print(prompt)\n",
    "    utils.get_top_responses(\n",
    "        prompt=prompt,\n",
    "        model=tl_model,\n",
    "        top_k=5,\n",
    "    )\n",
    "    if i == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = dict(\n",
    "    corr_base=PromptData.get_data(get_corr_prompt(question), tl_model),\n",
    "    corr_competing=PromptData.get_data(get_corr_prompt(question, correct=False), tl_model),\n",
    "    dontsay_base=PromptData.get_data(get_dontsay_prompt(question, \"apple\"), tl_model),\n",
    "    dontsay_competing=PromptData.get_data(get_dontsay_prompt(question, \"football\"), tl_model),\n",
    ")\n",
    "\n",
    "for prompt_type in [\"corr\", \"dontsay\"]:\n",
    "    base_data = prompt_dict[f\"{prompt_type}_base\"]\n",
    "    competing_data = prompt_dict[f\"{prompt_type}_competing\"]\n",
    "\n",
    "    print(f\"Logits for {prompt_type}_base\")\n",
    "    print(base_data.ml_str, base_data.ml_token, base_data.final_logits[base_data.ml_token].item())\n",
    "    print(competing_data.ml_str, competing_data.ml_token, base_data.final_logits[competing_data.ml_token].item())\n",
    "    print(f\"Logits for {prompt_type}_competing\")\n",
    "    print(base_data.ml_str, base_data.ml_token, competing_data.final_logits[base_data.ml_token].item())\n",
    "    print(competing_data.ml_str, competing_data.ml_token, competing_data.final_logits[competing_data.ml_token].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_base, cache_base = tl_model.run_with_cache(\n",
    "    get_corr_prompt(question, correct=True),\n",
    "    remove_batch_dim=True,\n",
    ")\n",
    "logits_competing, cache_competing = tl_model.run_with_cache(\n",
    "    get_corr_prompt(question, correct=False),\n",
    "    remove_batch_dim=True,\n",
    ")\n",
    "\n",
    "logits_base = tl_utils.remove_batch_dim(logits_base)\n",
    "logits_competing = tl_utils.remove_batch_dim(logits_competing)\n",
    "logits_base.shape, logits_competing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute lens data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_dict = {\n",
    "    k: LensData.get_data(\n",
    "        prompt_data=v,\n",
    "        tl_model=tl_model,\n",
    "        logit_lens=logit_lens,\n",
    "        tuned_lens=tuned_lens,\n",
    "    )\n",
    "    for k, v in prompt_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def subject_relation_attention(\n",
    "        cache: dict,\n",
    "        subject_index: int,\n",
    "        relation_index: int,\n",
    "        correct_indices: (int, int),\n",
    "):\n",
    "    total_list = []\n",
    "    for layer_num in range(32):\n",
    "        head_list = []\n",
    "        for head_num in range(32):\n",
    "            relation_attention = np.sum(cache[f\"blocks.{layer_num}.attn.hook_pattern\"][head_num, -1, relation_index:].cpu().numpy())\n",
    "            subject_attention = np.sum(cache[f\"blocks.{layer_num}.attn.hook_pattern\"][head_num, -1, subject_index:relation_index].cpu().numpy())\n",
    "            correct_attention = np.sum(cache[f\"blocks.{layer_num}.attn.hook_pattern\"][head_num, -1, correct_indices[0]:correct_indices[1]].cpu().numpy())\n",
    "            head_list.append((relation_attention, subject_attention, correct_attention))\n",
    "        total_list.append(head_list)\n",
    "    return total_list\n",
    "\n",
    "def topk_heads_attention(total_list, k=1):\n",
    "    # Flatten list and associate each attention value with its layer and head\n",
    "    flattened_list = [((layer_num, head_num), head_list[2]) for layer_num, layer in enumerate(total_list) for head_num, head_list in enumerate(layer)]\n",
    "    \n",
    "    # Sort based on correct attention\n",
    "    sorted_list = sorted(flattened_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top k\n",
    "    return sorted_list[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "total_list = subject_relation_attention(cache_competing, 40, 44, (25, 26))\n",
    "top_heads = topk_heads_attention(total_list, k=5)\n",
    "print(top_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split DLA by attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_token = tokenizer.encode(\"football\")[1]\n",
    "print(football_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_corr_prompt(question, correct=True)\n",
    "print(len(tokenizer.encode(prompt)))\n",
    "print(tokenizer.decode(tokenizer.encode(prompt)[40:44]))\n",
    "print(tokenizer.decode(tokenizer.encode(prompt)[0:40]))\n",
    "print(tokenizer.decode(tokenizer.encode(prompt)[44:52]))\n",
    "print(tokenizer.decode(tokenizer.encode(prompt)[25:27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dla = dla_attention_token_group(cache_base, tl_model, football_token, (0,40), (40,44), (44, 52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torch import einsum\n",
    "def dla_attention_token_group(\n",
    "    cache: ActivationCache,\n",
    "    model: HookedTransformer,\n",
    "    all_tokenized_attributes,\n",
    "    bos_token_indices,\n",
    "    subject_token_indices,\n",
    "    relation_token_indices,\n",
    ") -> Float[torch.Tensor, \"attribute model_component token_group\"]:\n",
    "    \"\"\"DLA by attention head, by token group that is attended to\n",
    "\n",
    "    The token groups are (bos, subject, relation).\n",
    "    \"\"\"\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    logit_directions: Float[\n",
    "        torch.Tensor, \"attribute d_model\"\n",
    "    ] = model.tokens_to_residual_directions(torch.tensor(all_tokenized_attributes))\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        value: Float[torch.Tensor, \"pos head_index d_head\"] = cache[\n",
    "            f\"blocks.{layer}.attn.hook_v\"\n",
    "        ][0]\n",
    "\n",
    "        pattern_post_softmax: Float[\n",
    "            torch.Tensor, \"head_index query_pos key_pos\"\n",
    "        ] = cache[f\"blocks.{layer}.attn.hook_pattern\"][0]\n",
    "        \n",
    "        # Z is usually calculated as values * attention, summed across keys\n",
    "        z = einsum(\n",
    "            \"key_pos head_index d_head, \\\n",
    "                head_index query_pos key_pos -> \\\n",
    "                query_pos key_pos head_index d_head\",\n",
    "            value,\n",
    "            pattern_post_softmax,\n",
    "        )\n",
    "\n",
    "        weights_output: Float[torch.Tensor, \"head_index d_head d_model\"] = model.blocks[\n",
    "            layer\n",
    "        ].attn.W_O\n",
    "\n",
    "        # bias_output: Float[torch.Tensor, \"d_model\"] = model.blocks[layer].attn.b_O\n",
    "\n",
    "        result = einsum(\n",
    "            \"query_pos key_pos head_index d_head, \\\n",
    "                    head_index d_head d_model -> \\\n",
    "                    query_pos key_pos head_index d_model\",\n",
    "            z,\n",
    "            weights_output,\n",
    "        )\n",
    "\n",
    "        # Stacked head result (pos slice on last token)\n",
    "        result_on_final_token: Float[\n",
    "            torch.Tensor, \"key_pos head_index d_model\"\n",
    "        ] = result[-1]\n",
    "\n",
    "        # Scale\n",
    "        if model.cfg.normalization_type not in [\"LN\", \"LNPre\"]:\n",
    "            scaled_result = result_on_final_token\n",
    "        else:\n",
    "            center_stack = result_on_final_token - result_on_final_token.mean(\n",
    "                dim=-1, keepdim=True\n",
    "            )\n",
    "            scale = cache[\"ln_final.hook_scale\"][0, -1, :]  # first batch, last token\n",
    "            scaled_result = center_stack / scale\n",
    "\n",
    "        logit_attrs = einsum(\n",
    "            \"key_pos head_index d_model, attribute d_model -> key_pos head_index attribute\",\n",
    "            scaled_result,\n",
    "            logit_directions,\n",
    "        )\n",
    "\n",
    "        logit_attrs = einops.rearrange(\n",
    "            logit_attrs, \"key_pos head_index attribute -> attribute head_index key_pos\"\n",
    "        )\n",
    "\n",
    "        # Sum by category (bos, subject, relation)\n",
    "        bos_token_attribution: Float[\n",
    "            torch.Tensor, \"attribute head_index\"\n",
    "        ] = logit_attrs[:, :, bos_token_indices].sum(dim=-1)\n",
    "        subject_tokens_attribution: Float[\n",
    "            torch.Tensor, \"attribute head_index\"\n",
    "        ] = logit_attrs[:, :, subject_token_indices].sum(dim=-1)\n",
    "        relation_token_attribution: Float[\n",
    "            torch.Tensor, \"attribute head_index\"\n",
    "        ] = logit_attrs[:, :, relation_token_indices].sum(dim=-1)\n",
    "\n",
    "        combined: Float[torch.Tensor, \"attribute head_index token_group\"] = torch.stack(\n",
    "            [\n",
    "                bos_token_attribution,\n",
    "                subject_tokens_attribution,\n",
    "                relation_token_attribution,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        layers.append(combined)\n",
    "\n",
    "    layers_tensor: Float[\n",
    "        torch.Tensor, \"layer attribute head_index token_group\"\n",
    "    ] = torch.stack(layers, dim=0)\n",
    "\n",
    "    rearranged = einops.rearrange(\n",
    "        layers_tensor,\n",
    "        \"layer attribute head_index token_group -> attribute (layer head_index) token_group\",\n",
    "    )\n",
    "\n",
    "    return rearranged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot by head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_base = lens_dict[\"corr_base\"]\n",
    "ld_comp = lens_dict[\"corr_competing\"]\n",
    "\n",
    "logits_base = ld_base.get_logits(\"attn_head\", lens_type=\"tuned\")\n",
    "logits_comp = ld_comp.get_logits(\"attn_head\", lens_type=\"tuned\")\n",
    "\n",
    "utils.plot_head_data(\n",
    "    lines=[\n",
    "        (\n",
    "            f\"logit: |{ld_base.pd.ml_str}|\", \n",
    "            logits_base[:, ld_base.pd.ml_token]\n",
    "        ),\n",
    "        (\n",
    "            f\"logit: |{ld_comp.pd.ml_str}|\", \n",
    "            logits_base[:, ld_comp.pd.ml_token]\n",
    "        ),\n",
    "    ],\n",
    "    annotation_text=ld_base.pd.prompt.split(\"\\n\")[-1],\n",
    "    title=\"Answer correctly prompt\",\n",
    "    yaxis_title=\"Logit\",\n",
    ").show()\n",
    "\n",
    "utils.plot_head_data(\n",
    "    lines=[\n",
    "        (\n",
    "            f\"logit: |{ld_base.pd.ml_str}|\", \n",
    "            logits_comp[:, ld_base.pd.ml_token]\n",
    "        ),\n",
    "        (\n",
    "            f\"logit: |{ld_comp.pd.ml_str}|\", \n",
    "            logits_comp[:, ld_comp.pd.ml_token]\n",
    "        ),\n",
    "    ],\n",
    "    annotation_text=ld_comp.pd.prompt.split(\"\\n\")[-1],\n",
    "    title=\"Answer incorrectly prompt\",\n",
    "    yaxis_title=\"Logit\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_base = lens_dict[\"dontsay_base\"]\n",
    "ld_comp = lens_dict[\"dontsay_competing\"]\n",
    "\n",
    "logits_base = ld_base.get_logits(\"attn_head\", lens_type=\"tuned\")\n",
    "logits_comp = ld_comp.get_logits(\"attn_head\", lens_type=\"tuned\")\n",
    "\n",
    "utils.plot_head_data(\n",
    "    lines=[\n",
    "        (\n",
    "            f\"logit: |{ld_base.pd.ml_str}|\", \n",
    "            logits_base[:, ld_base.pd.ml_token]\n",
    "        ),\n",
    "        (\n",
    "            f\"logit: |{ld_comp.pd.ml_str}|\", \n",
    "            logits_base[:, ld_comp.pd.ml_token]\n",
    "        ),\n",
    "    ],\n",
    "    annotation_text=ld_base.pd.prompt.split(\"\\n\")[-1],\n",
    "    title=\"Don't say apple\",\n",
    "    yaxis_title=\"Logit\",\n",
    ").show()\n",
    "\n",
    "utils.plot_head_data(\n",
    "    lines=[\n",
    "        (\n",
    "            f\"logit: |{ld_base.pd.ml_str}|\", \n",
    "            logits_comp[:, ld_base.pd.ml_token]\n",
    "        ),\n",
    "        (\n",
    "            f\"logit: |{ld_comp.pd.ml_str}|\", \n",
    "            logits_comp[:, ld_comp.pd.ml_token]\n",
    "        ),\n",
    "    ],\n",
    "    annotation_text=ld_comp.pd.prompt.split(\"\\n\")[-1],\n",
    "    title=\"Don't say football\",\n",
    "    yaxis_title=\"Logit\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = tl_model.to_str_tokens(get_corr_prompt(question, correct=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = tl_model.to_str_tokens(get_corr_prompt(question, correct=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_incorrect.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(31, -1, -1):\n",
    "    attention_pattern = cache_incorrect[f\"blocks.{i}.attn.hook_pattern\"]\n",
    "\n",
    "    print(attention_pattern.shape)\n",
    "    print(f\"Layer {i} Head Attention Patterns:\")\n",
    "    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
