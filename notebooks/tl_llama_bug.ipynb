{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show that the transformer-lens implementation of Llama does not match that of Huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884b5711b1384713bbc83fa99d613d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf464b9a8f94037bf1661e7c5133091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32,\n",
    ").cpu()\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    hf_model=AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float32,\n",
    "    ),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    "    device=\"cpu\",\n",
    "    n_devices=1,\n",
    "    move_to_device=True,\n",
    "    fold_ln=False,\n",
    "    fold_value_biases=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2226,  0.0299,  0.2729,  ...,  1.4124,  1.9937,  0.7167],\n",
      "         [-7.5956, -1.9652, -1.2686,  ..., -6.3465, -4.6871, -7.5035],\n",
      "         [-3.2299,  2.0011,  6.4700,  ..., -1.7174, -2.2347, -2.4737],\n",
      "         [-8.5657, -5.2156,  5.2663,  ..., -3.5372, -2.7532, -4.0162]]]) tensor([[[ 0.2707,  0.0165,  0.2806,  ...,  1.4403,  2.0234,  0.7647],\n",
      "         [-7.5216, -2.1810, -1.1470,  ..., -6.3703, -4.6442, -7.4660],\n",
      "         [-3.6662,  1.9481,  6.2992,  ..., -2.1683, -2.4888, -2.6284],\n",
      "         [-8.7830, -5.1027,  5.4732,  ..., -3.7767, -2.9180, -4.4215]]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mallclose(tl_logits, hf_logits, atol\u001b[39m=\u001b[39matol)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     check_similarity_with_hf_model(tl_model, hf_model, atol\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m hf_logits \u001b[39m=\u001b[39m hf_model(tokens)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(tl_logits, hf_logits)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/tl_llama_bug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mallclose(tl_logits, hf_logits, atol\u001b[39m=\u001b[39matol)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def check_similarity_with_hf_model(\n",
    "    tl_model: HookedTransformer,\n",
    "    hf_model: AutoModelForCausalLM,\n",
    "    atol: float,\n",
    "    prompt: str = \"Hello world!\",\n",
    "):\n",
    "    tokens = tl_model.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    tl_logits = tl_model(tokens, prepend_bos=False)\n",
    "    hf_logits = hf_model(tokens).logits\n",
    "\n",
    "    print(tl_logits, hf_logits)\n",
    "    assert torch.allclose(tl_logits, hf_logits, atol=atol)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    check_similarity_with_hf_model(tl_model, hf_model, atol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vicuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
