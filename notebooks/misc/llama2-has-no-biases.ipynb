{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6da7f76c6e40e4b9cc80542553d8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transformerlens model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281663597a5a4b07b226570608e80fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hf_model_cpu = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    hf_model=hf_model_cpu,\n",
    "    device=\"cuda\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 32000]) torch.Size([1, 6, 32000])\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"The capital of Germany is\"\n",
    "TOKENS = tokenizer(PROMPT, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    LOGITS_HF = hf_model(TOKENS).logits.clone()\n",
    "    LOGITS_TL = tl_model(TOKENS)\n",
    "\n",
    "print(LOGITS_HF.shape, LOGITS_TL.shape)\n",
    "assert torch.allclose(LOGITS_HF, LOGITS_TL, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mier/.micromamba/envs/pii/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mier/.micromamba/envs/pii/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The capital of Germany is Berlin. Berlin is the largest city in Germany and is known for its rich history, cultural attractions, and vibrant nightlife. The city is home to many famous landmarks, including the Berlin Wall, the Brandenburg Gate, and the Berlin Cathedral. Berlin is also a hub for art and culture, with numerous museums, galleries, and festivals throughout the year.\n",
      "The city has a long and complex history, having been the capital of Prussia, the German Empire, and the Weimar Republic before becoming the capital of the Federal Republic of Germany in 1949. Berlin has been influenced by many different cultures throughout its history, including the Slavic peoples, the Franks, the Teutonic Knights, and the Soviet Union.\n",
      "Today, Berlin is a thriving metropolis with a diverse population and a strong economy. The city is home to many international companies and organizations, and is a popular destination for tourists and expats alike. Berlin is also known for its vibrant nightlife, with a wide range of bars, clubs, and live music venues to choose from.\n",
      "In addition to its cultural attractions and nightlife, Berlin is also a great place to visit for outdoor activities. The city has many parks and green spaces, including the famous Tiergarten, which is one of the largest urban parks in Europe. Berlin is also home to several lakes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1876a0cb5344d8a6c7985e777154e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_new_tokens = 300\n",
    "with torch.no_grad():\n",
    "    COMPL_HF = tl_model.to_string(\n",
    "        hf_model.generate(\n",
    "            TOKENS,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )[0]\n",
    "    )\n",
    "    print(COMPL_HF)\n",
    "\n",
    "    COMPL_TL = tl_model.generate(\n",
    "        PROMPT, max_new_tokens=max_new_tokens, temperature=0\n",
    "    )\n",
    "\n",
    "assert COMPL_HF[:4] == \"<s> \"\n",
    "assert COMPL_HF[4:] == COMPL_TL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
