{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we study the behavior of heads L18H9, L27H29, and L26H9 on the final token of dontsay prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import circuitsvis.attention as cv_attention\n",
    "import circuitsvis.tokens as cv_tokens\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "import transformer_lens.utils as tl_utils\n",
    "from fancy_einsum import einsum\n",
    "from jaxtyping import Float\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tuned_lens.nn import LogitLens, TunedLens\n",
    "\n",
    "from pii import prompt_data, utils\n",
    "from pii.subnet_flow.masked_model import PerTokenMaskedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad0f265125345a4b8343dc1a8e9e76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb28d5a8cbb142889b888c200f9bc621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Berlin. Berlin is the largest city in Germany and is known for its rich history, cultural attractions\n"
     ]
    }
   ],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    hf_model=hf_model,\n",
    "    device=\"cuda\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    print(\n",
    "        tl_model.generate(\n",
    "            \"The capital of Germany is\", max_new_tokens=20, temperature=0\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an obedient assistant who only responds with a single word. You are not allowed to say the forbidden word football.\n",
      "<</SYS>>\n",
      "An example of a red and crunchy fruit is a [/INST] \n",
      "Rank 0. Logit: 20.62 Prob: 85.45% Tokens: (12113) |Apple|</s>|\n",
      "Rank 1. Logit: 18.48 Prob: 10.05% Tokens: (26163) |apple|</s>|\n",
      "Rank 2. Logit: 16.57 Prob:  1.48% Tokens: ( 2401) |App|les|</s>|\n",
      "Rank 3. Logit: 16.55 Prob:  1.46% Tokens: (  624) |St|raw|berry|</s>|\n",
      "Rank 4. Logit: 15.66 Prob:  0.60% Tokens: (26048) |Orange|</s>|\n",
      "\n",
      "[INST] <<SYS>>\n",
      "You are an obedient assistant who only responds with a single word. You are not allowed to say the forbidden word apple.\n",
      "<</SYS>>\n",
      "An example of a red and crunchy fruit is a [/INST] \n",
      "Rank 0. Logit: 16.14 Prob: 36.70% Tokens: (26048) |Orange|</s>|\n",
      "Rank 1. Logit: 15.47 Prob: 18.88% Tokens: ( 3938) |Pe|ach|</s>|\n",
      "Rank 2. Logit: 15.00 Prob: 11.80% Tokens: (21265) |Pear|</s>|\n",
      "Rank 3. Logit: 14.62 Prob:  8.07% Tokens: (24841) |orange|</s>|\n",
      "Rank 4. Logit: 14.28 Prob:  5.72% Tokens: (14954) |cher|ry|</s>|\n"
     ]
    }
   ],
   "source": [
    "def get_dontsay_prompt(question: str, forbidden_word: str):\n",
    "    return f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are an obedient assistant who only responds with a single word. \\\n",
    "You are not allowed to say the forbidden word {forbidden_word}.\n",
    "<</SYS>>\n",
    "{question} [/INST] \\\n",
    "\"\"\"\n",
    "\n",
    "# question = \"The protagonist of Half Life is\"\n",
    "# forbidden_words = [\"apple\", \"Gordon\"]\n",
    "# question = \"Tom Brady plays the sport of\"\n",
    "# forbidden_words = [\"apple\", \"football\"]\n",
    "# question = \"The element with atomic symbol H is\"\n",
    "# forbidden_words = [\"apple\", \"hydrogen\"]\n",
    "\n",
    "question = \"An example of a red and crunchy fruit is a\"\n",
    "forbidden_words = [\"football\", \"apple\"]\n",
    "\n",
    "tl_model.remove_all_hook_fns(including_permanent=True)\n",
    "for i, forbidden_word in enumerate(forbidden_words):\n",
    "    prompt = get_dontsay_prompt(question, forbidden_word)\n",
    "    print(prompt)\n",
    "    utils.get_top_responses(\n",
    "        prompt=prompt,\n",
    "        model=tl_model,\n",
    "        top_k=5,\n",
    "        n_continuation_tokens=10,\n",
    "    )\n",
    "    if i == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = dict(\n",
    "    dontsay_base=prompt_data.PromptData.get_data(get_dontsay_prompt(question, forbidden_words[0]), tl_model),\n",
    "    dontsay_competing=prompt_data.PromptData.get_data(get_dontsay_prompt(question, forbidden_words[1]), tl_model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_base = prompt_dict[\"dontsay_base\"]\n",
    "cache = pd_base.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[\"resid_mid\", 10].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
