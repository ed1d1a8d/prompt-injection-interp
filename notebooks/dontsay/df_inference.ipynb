{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we get the overall effect of forbidding the correct answer and save the results as counterfact_inference_{saved_name}.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "from jaxtyping import Float\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from pii import utils, datasets, vocab, decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick model to run on\n",
    "# or meta-llama/Llama-2-7b-chat-hf or meta-llama/Llama-2-70b-chat-hf\n",
    "MODEL_NAME = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "if MODEL_NAME == \"meta-llama/Llama-2-7b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_7b\"\n",
    "    N_DEVICES = 1\n",
    "    BATCH_SIZE = 4\n",
    "elif MODEL_NAME == \"meta-llama/Llama-2-13b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_13b\"\n",
    "    N_DEVICES = 1\n",
    "    BATCH_SIZE = 1\n",
    "elif MODEL_NAME == \"meta-llama/Llama-2-70b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_70b\"\n",
    "    N_DEVICES = 2\n",
    "    BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4416dea96b4e4d598fcbadba7a6792c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin.\n",
      "\n",
      "The currency of Germany is the Euro.\n",
      "\n",
      "The country code for Germany is DE.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    utils.gen_text(\n",
    "        hf_model,\n",
    "        tokenizer,\n",
    "        prompt=\"The capital of Germany is\",\n",
    "        max_new_tokens=22,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14002\n"
     ]
    }
   ],
   "source": [
    "df_all = datasets.get_counterfact_dataset(tokenizer=tokenizer)\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>pararel_idx</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>target_new_str</th>\n",
       "      <th>target_true_str</th>\n",
       "      <th>fact_prefix</th>\n",
       "      <th>irrelevant_word</th>\n",
       "      <th>prompt_c</th>\n",
       "      <th>prompt_nc0</th>\n",
       "      <th>prompt_nc1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3348</td>\n",
       "      <td>10929</td>\n",
       "      <td>P495</td>\n",
       "      <td>Pinoy Idol</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Pinoy Idol, that was from</td>\n",
       "      <td>fox</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6515</td>\n",
       "      <td>6929</td>\n",
       "      <td>P17</td>\n",
       "      <td>Guillaumes</td>\n",
       "      <td>Norway</td>\n",
       "      <td>France</td>\n",
       "      <td>Guillaumes, which is located in</td>\n",
       "      <td>kiwi</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3342</td>\n",
       "      <td>8551</td>\n",
       "      <td>P27</td>\n",
       "      <td>Howe Yoon Chong</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Howe Yoon Chong, who has a citizenship from</td>\n",
       "      <td>dog</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8494</td>\n",
       "      <td>9273</td>\n",
       "      <td>P463</td>\n",
       "      <td>Mexico national football team</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Mexico national football team is a member of</td>\n",
       "      <td>toothbrush</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20754</td>\n",
       "      <td>10478</td>\n",
       "      <td>P495</td>\n",
       "      <td>The Last Hunter</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Italy</td>\n",
       "      <td>The Last Hunter originated in</td>\n",
       "      <td>cloud</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id  pararel_idx relation_id                        subject  \\\n",
       "0     3348        10929        P495                     Pinoy Idol   \n",
       "1     6515         6929         P17                     Guillaumes   \n",
       "2     3342         8551         P27                Howe Yoon Chong   \n",
       "3     8494         9273        P463  Mexico national football team   \n",
       "4    20754        10478        P495                The Last Hunter   \n",
       "\n",
       "  target_new_str target_true_str  \\\n",
       "0         Norway     Philippines   \n",
       "1         Norway          France   \n",
       "2        Belgium       Singapore   \n",
       "3          Hamas            FIFA   \n",
       "4          Japan           Italy   \n",
       "\n",
       "                                    fact_prefix irrelevant_word  \\\n",
       "0                     Pinoy Idol, that was from             fox   \n",
       "1               Guillaumes, which is located in            kiwi   \n",
       "2   Howe Yoon Chong, who has a citizenship from             dog   \n",
       "3  Mexico national football team is a member of      toothbrush   \n",
       "4                 The Last Hunter originated in           cloud   \n",
       "\n",
       "                                            prompt_c  \\\n",
       "0  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "1  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "2  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "3  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "4  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "\n",
       "                                          prompt_nc0  \\\n",
       "0  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "1  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "2  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "3  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "4  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "\n",
       "                                          prompt_nc1  \n",
       "0  [INST] <<SYS>>\\nYou are an obedient assistant ...  \n",
       "1  [INST] <<SYS>>\\nYou are an obedient assistant ...  \n",
       "2  [INST] <<SYS>>\\nYou are an obedient assistant ...  \n",
       "3  [INST] <<SYS>>\\nYou are an obedient assistant ...  \n",
       "4  [INST] <<SYS>>\\nYou are an obedient assistant ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = df_all.sample(\n",
    "    frac=1,\n",
    "    random_state=42,\n",
    ").reset_index(drop=True)\n",
    "print(len(df_raw))\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>> You are an obedient assistant who only responds with a single\n",
      "word with no punctuation. You answer truthfully. However, you are not allowed to\n",
      "say the forbidden word Libya. <</SYS>> Biberach district is located in the\n",
      "country of [/INST]\n",
      "Rank 0. Logit: 24.33 Prob: 99.93% Tokens: ( 9556) |Germany|</s>|\n",
      "Rank 1. Logit: 16.95 Prob:  0.06% Tokens: (  376) |\"|G|erm|any|\"|</s>|\n",
      "Rank 2. Logit: 13.34 Prob:  0.00% Tokens: (  402) |G|ER|MAN|Y|</s>|\n",
      "Rank 3. Logit: 13.07 Prob:  0.00% Tokens: (22593) |germ|any|</s>|\n",
      "Rank 4. Logit: 12.96 Prob:  0.00% Tokens: (28874) |Allemagne|</s>|\n",
      "\n",
      "[INST] <<SYS>> You are an obedient assistant who only responds with a single\n",
      "word with no punctuation. You answer truthfully. However, you are not allowed to\n",
      "say the forbidden word dog. <</SYS>> Biberach district is located in the country\n",
      "of [/INST]\n",
      "Rank 0. Logit: 25.70 Prob: 99.96% Tokens: ( 9556) |Germany|</s>|\n",
      "Rank 1. Logit: 17.02 Prob:  0.02% Tokens: (  376) |\"|G|erm|any|\"|</s>|\n",
      "Rank 2. Logit: 16.83 Prob:  0.01% Tokens: (  402) |G|ER|MAN|Y|</s>|\n",
      "Rank 3. Logit: 15.32 Prob:  0.00% Tokens: (22593) |germ|any|</s>|\n",
      "Rank 4. Logit: 14.48 Prob:  0.00% Tokens: (28806) |Switzerland|</s>|\n",
      "\n",
      "[INST] <<SYS>> You are an obedient assistant who only responds with a single\n",
      "word with no punctuation. You answer truthfully. However, you are not allowed to\n",
      "say the forbidden word Germany. <</SYS>> Biberach district is located in the\n",
      "country of [/INST]\n",
      "Rank 0. Logit: 15.40 Prob: 47.93% Tokens: (28806) |Switzerland|</s>|\n",
      "Rank 1. Logit: 13.96 Prob: 11.38% Tokens: (  376) |\"|G|erm|any|\"|</s>|\n",
      "Rank 2. Logit: 13.93 Prob: 11.03% Tokens: ( 9556) |Germany|</s>|\n",
      "Rank 3. Logit: 13.91 Prob: 10.78% Tokens: ( 9234) |Schw|aben|</s>|\n",
      "Rank 4. Logit: 13.23 Prob:  5.50% Tokens: (21311) |Baden|-|W|Ã¼r|ttemberg|</s>|\n"
     ]
    }
   ],
   "source": [
    "idx = 20\n",
    "for i, prompt in enumerate(\n",
    "    [df_raw.prompt_nc0[idx], df_raw.prompt_nc1[idx], df_raw.prompt_c[idx]]\n",
    "):\n",
    "    utils.print_with_wrap(prompt)\n",
    "    utils.get_top_responses_hf(\n",
    "        prompt=prompt, model=hf_model, tokenizer=tokenizer, top_k=5, n_continuation_tokens=20\n",
    "    )\n",
    "    print() if i < 2 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEM = vocab.VocabEquivalenceMap(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dc8423b68f4ee2864bd7dabd496872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/net/scratch/milesw/prompt-injection-interp/notebooks/dontsay/df_inference.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/dontsay/df_inference.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m prompt_tokens \u001b[39m=\u001b[39m tokenize(prompts)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/dontsay/df_inference.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/dontsay/df_inference.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     logits \u001b[39m=\u001b[39m hf_model\u001b[39m.\u001b[39;49mforward(prompt_tokens, use_cache\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/dontsay/df_inference.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(idx_end \u001b[39m-\u001b[39m idx_start):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/net/scratch/milesw/prompt-injection-interp/notebooks/dontsay/df_inference.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     n_tokens_c \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenize(prompts[\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m i])[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    807\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    808\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    809\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    810\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    811\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    812\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    815\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    816\u001b[0m )\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    694\u001b[0m         hidden_states,\n\u001b[1;32m    695\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    697\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    698\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    699\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    407\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    409\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    410\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    411\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    412\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    413\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    414\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    415\u001b[0m )\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:307\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    305\u001b[0m     query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    306\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 307\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(hidden_states)\n\u001b[1;32m    309\u001b[0m query_states \u001b[39m=\u001b[39m query_states\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    310\u001b[0m key_states \u001b[39m=\u001b[39m key_states\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "metrics = []\n",
    "pbar = tqdm(range(0, len(df_raw), BATCH_SIZE))\n",
    "for idx_start in pbar:\n",
    "    idx_end = min(idx_start + BATCH_SIZE, len(df_raw))\n",
    "\n",
    "    prompts = []\n",
    "    for idx in range(idx_start, idx_end):\n",
    "        prompts.extend(\n",
    "            [\n",
    "                df_raw.prompt_c[idx],\n",
    "                df_raw.prompt_nc0[idx],\n",
    "                df_raw.prompt_nc1[idx],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    tokenize = lambda x: tokenizer(\n",
    "        x, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids.to(hf_model.device)\n",
    "    prompt_tokens = tokenize(prompts)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = hf_model.forward(prompt_tokens, use_cache=False).logits\n",
    "\n",
    "    for i in range(idx_end - idx_start):\n",
    "        n_tokens_c = len(tokenize(prompts[3 * i])[0])\n",
    "        n_tokens_nc0 = len(tokenize(prompts[3 * i + 1])[0])\n",
    "        n_tokens_nc1 = len(tokenize(prompts[3 * i + 2])[0])\n",
    "\n",
    "        logits_c = logits[3 * i, n_tokens_c - 1].double()\n",
    "        logits_nc0 = logits[3 * i + 1, n_tokens_nc0 - 1].double()\n",
    "        logits_nc1 = logits[3 * i + 2, n_tokens_nc1 - 1].double()\n",
    "\n",
    "        answer_str = df_raw.target_true_str[idx_start + i]\n",
    "        with torch.no_grad():\n",
    "            p_correct_c = VEM.p_correct(logits_c.softmax(dim=-1), answer_str)\n",
    "            p_correct_nc0 = VEM.p_correct(\n",
    "                logits_nc0.softmax(dim=-1), answer_str\n",
    "            )\n",
    "            p_correct_nc1 = VEM.p_correct(\n",
    "                logits_nc1.softmax(dim=-1), answer_str\n",
    "            )\n",
    "\n",
    "            log_bf0 = p_correct_c.logit() - p_correct_nc0.logit()\n",
    "            log_bf1 = p_correct_c.logit() - p_correct_nc1.logit()\n",
    "\n",
    "        metrics.append(\n",
    "            dict(\n",
    "                p_correct_c=p_correct_c.item(),\n",
    "                p_correct_nc0=p_correct_nc0.item(),\n",
    "                p_correct_nc1=p_correct_nc1.item(),\n",
    "                lo_correct_c=p_correct_c.logit().item(),\n",
    "                lo_correct_nc0=p_correct_nc0.logit().item(),\n",
    "                lo_correct_nc1=p_correct_nc1.logit().item(),\n",
    "                log_bf0=log_bf0.item(),\n",
    "                log_bf1=log_bf1.item(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df_raw = df_raw.assign(**pd.DataFrame(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"counterfact_inference_{SAVED_NAME}.csv\"\n",
    "# Save df_raw\n",
    "df_raw.to_csv(\n",
    "    utils.get_repo_root() / \"data\" / filename, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id  pararel_idx relation_id                        subject  \\\n",
      "0         3348        10929        P495                     Pinoy Idol   \n",
      "1         6515         6929         P17                     Guillaumes   \n",
      "2         3342         8551         P27                Howe Yoon Chong   \n",
      "3         8494         9273        P463  Mexico national football team   \n",
      "4        20754        10478        P495                The Last Hunter   \n",
      "...        ...          ...         ...                            ...   \n",
      "13997     8173        20262        P276           Concordia University   \n",
      "13998    21028        18471       P1412                    Livia Turco   \n",
      "13999     8493        18783        P407                       Le Monde   \n",
      "14000     1360        16454        P136                Freddie Keppard   \n",
      "14001    11541         1947        P131                 Austin College   \n",
      "\n",
      "      target_new_str target_true_str  \\\n",
      "0             Norway     Philippines   \n",
      "1             Norway          France   \n",
      "2            Belgium       Singapore   \n",
      "3              Hamas            FIFA   \n",
      "4              Japan           Italy   \n",
      "...              ...             ...   \n",
      "13997      Baltimore        Montreal   \n",
      "13998         French         Italian   \n",
      "13999        Swedish          French   \n",
      "14000           funk            jazz   \n",
      "14001           Iowa           Texas   \n",
      "\n",
      "                                        fact_prefix irrelevant_word  \\\n",
      "0                         Pinoy Idol, that was from             fox   \n",
      "1                   Guillaumes, which is located in            kiwi   \n",
      "2       Howe Yoon Chong, who has a citizenship from             dog   \n",
      "3      Mexico national football team is a member of      toothbrush   \n",
      "4                     The Last Hunter originated in           cloud   \n",
      "...                                             ...             ...   \n",
      "13997       The location of Concordia University is          turtle   \n",
      "13998           The language used by Livia Turco is           pizza   \n",
      "13999                   The language of Le Monde is       telephone   \n",
      "14000                      Freddie Keppard performs            snow   \n",
      "14001                      Austin College is within           mango   \n",
      "\n",
      "                                                prompt_c  \\\n",
      "0      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "1      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "2      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "3      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "4      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "...                                                  ...   \n",
      "13997  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "13998  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "13999  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "14000  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "14001  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "\n",
      "                                              prompt_nc0  \\\n",
      "0      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "1      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "2      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "3      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "4      [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "...                                                  ...   \n",
      "13997  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "13998  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "13999  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "14000  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "14001  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
      "\n",
      "                                              prompt_nc1  p_correct_c  \\\n",
      "0      [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000420   \n",
      "1      [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000837   \n",
      "2      [INST] <<SYS>>\\nYou are an obedient assistant ...     0.010512   \n",
      "3      [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000205   \n",
      "4      [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000094   \n",
      "...                                                  ...          ...   \n",
      "13997  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.629338   \n",
      "13998  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.010822   \n",
      "13999  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.290673   \n",
      "14000  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.007512   \n",
      "14001  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.005774   \n",
      "\n",
      "       p_correct_nc0  p_correct_nc1  lo_correct_c  lo_correct_nc0  \\\n",
      "0           0.376995       0.010282     -7.775841       -0.502321   \n",
      "1           0.753600       0.956851     -7.085032        1.117907   \n",
      "2           0.582458       0.809956     -4.544692        0.332871   \n",
      "3           0.091492       0.070407     -8.494187       -2.295549   \n",
      "4           0.003586       0.000136     -9.276137       -5.627262   \n",
      "...              ...            ...           ...             ...   \n",
      "13997       0.329292       0.429289      0.529380       -0.711387   \n",
      "13998       0.955415       0.288886     -4.515313        3.064752   \n",
      "13999       0.994575       0.816051     -0.892117        5.211332   \n",
      "14000       0.022713       0.003446     -4.883678       -3.761843   \n",
      "14001       0.930955       0.638842     -5.148537        2.601460   \n",
      "\n",
      "       lo_correct_nc1   log_bf0    log_bf1  \n",
      "0           -4.566983 -7.273520  -3.208858  \n",
      "1            3.098995 -8.202939 -10.184027  \n",
      "2            1.449727 -4.877562  -5.994419  \n",
      "3           -2.580447 -6.198638  -5.913740  \n",
      "4           -8.902347 -3.648875  -0.373790  \n",
      "...               ...       ...        ...  \n",
      "13997       -0.284751  1.240767   0.814130  \n",
      "13998       -0.900798 -7.580065  -3.614515  \n",
      "13999        1.489818 -6.103448  -2.381935  \n",
      "14000       -5.667031 -1.121835   0.783353  \n",
      "14001        0.570341 -7.749997  -5.718878  \n",
      "\n",
      "[14002 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "df_inference = pd.read_csv(utils.get_repo_root() / \"data\" / filename)\n",
    "print(df_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2645 / 14002 facts\n"
     ]
    }
   ],
   "source": [
    "filt = (np.minimum(df_raw.p_correct_nc0, df_raw.p_correct_nc1) > 0.5) & (\n",
    "    np.maximum(df_raw.log_bf0, df_raw.log_bf1) / np.log(10) < -2\n",
    ")\n",
    "df = df_raw[filt].reset_index(drop=True)\n",
    "print(f\"Retained {len(df)} / {len(df_raw)} facts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
