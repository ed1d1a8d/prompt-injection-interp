{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we get the overall effect of ablating each component and save the results as counterfact_ablation_{saved_name}.pkl, based on the df generated by df_inference.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizerFast\n",
    "\n",
    "from pii import utils, vocab, decomp_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick model to run on\n",
    "# or meta-llama/Llama-2-7b-chat-hf or meta-llama/Llama-2-70b-chat-hf\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "if MODEL_NAME == \"meta-llama/Llama-2-7b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_7b\"\n",
    "    BATCH_SIZE = 8\n",
    "    DTYPE = torch.float16\n",
    "    DEVICE_MAP = None\n",
    "elif MODEL_NAME == \"meta-llama/Llama-2-13b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_13b\"\n",
    "    BATCH_SIZE = 1\n",
    "    DTYPE = torch.float16\n",
    "    DEVICE_MAP = \"auto\"\n",
    "elif MODEL_NAME == \"meta-llama/Llama-2-70b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_70b\"\n",
    "    BATCH_SIZE = 16\n",
    "    DTYPE = torch.float16\n",
    "    DEVICE_MAP = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   huggingface-cli login\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "hf_model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=DEVICE_MAP,\n",
    ").eval()\n",
    "if DEVICE_MAP is None:\n",
    "    hf_model = hf_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    utils.gen_text(\n",
    "        hf_model,\n",
    "        tokenizer,\n",
    "        prompt=\"The capital of Germany is\",\n",
    "        max_new_tokens=22,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"counterfact_inference_{SAVED_NAME}.csv\"\n",
    "df_inference = pd.read_csv(utils.get_repo_root() / \"data\" / filename)\n",
    "df_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (np.minimum(df_inference.p_correct_nc0, df_inference.p_correct_nc1) > 0.5) & (\n",
    "    np.maximum(df_inference.log_bf0, df_inference.log_bf1) / np.log(10) < -2\n",
    ")\n",
    "df = df_inference[filt].reset_index(drop=True)\n",
    "print(f\"Retained {len(df)} / {len(df_inference)} facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of components (first-order-ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEM = vocab.VocabEquivalenceMap(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS: list[str] | None = None\n",
    "metrics = []\n",
    "\n",
    "pbar = tqdm(range(0, len(df), BATCH_SIZE))\n",
    "for idx_start in pbar:\n",
    "    idx_end = min(idx_start + BATCH_SIZE, len(df))\n",
    "\n",
    "    prompts = []\n",
    "    n_tokens_all = []\n",
    "    for idx in range(idx_start, idx_end):\n",
    "        prompts.extend(\n",
    "            [\n",
    "                df.prompt_c[idx],\n",
    "                df.prompt_nc0[idx],\n",
    "                df.prompt_nc1[idx],\n",
    "            ]\n",
    "        )\n",
    "        n_tokens_all.extend(\n",
    "            [\n",
    "                len(tokenizer(df.prompt_c[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc0[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc1[idx])[0]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    tokenize = lambda x: tokenizer(\n",
    "        x, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids.to(hf_model.device)\n",
    "    prompt_tokens = tokenize(prompts)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        comp_list = decomp_hf.get_all_resid_components_hf(\n",
    "            model=hf_model,\n",
    "            tokens=prompt_tokens,\n",
    "            resid_pos_idxs=[n - 1 for n in n_tokens_all],\n",
    "        )\n",
    "\n",
    "    for i in range(idx_end - idx_start):\n",
    "        n_tokens = n_tokens_all[3 * i : 3 * i + 3]\n",
    "        answer_str = df.target_true_str[idx_start + i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lc_c, lc_nc0, lc_nc1 = comp_list[3 * i : 3 * i + 3]\n",
    "            LABELS = lc_c.labels\n",
    "\n",
    "            res_final_c, res_final_nc0, res_final_nc1 = (\n",
    "                comp_list[3 * i + j].resid_post for j in range(3)\n",
    "            )\n",
    "\n",
    "            delta_c_nc0 = lc_nc0.components - lc_c.components\n",
    "            delta_c_nc1 = lc_nc1.components - lc_c.components\n",
    "            delta_nc0_c = lc_c.components - lc_nc0.components\n",
    "            delta_nc1_c = lc_c.components - lc_nc1.components\n",
    "\n",
    "            get_probs = lambda reses: VEM.p_correct(\n",
    "                hf_model.lm_head(hf_model.model.norm(reses))\n",
    "                .double()\n",
    "                .softmax(dim=-1),\n",
    "                correct_answer=answer_str,\n",
    "            ).to(hf_model.device)\n",
    "\n",
    "            p_correct_c_nc0 = get_probs(res_final_c[None, :] + delta_c_nc0)\n",
    "            p_correct_c_nc1 = get_probs(res_final_c[None, :] + delta_c_nc1)\n",
    "            p_correct_nc0_c = get_probs(res_final_nc0[None, :] + delta_nc0_c)\n",
    "            p_correct_nc1_c = get_probs(res_final_nc1[None, :] + delta_nc1_c)\n",
    "\n",
    "            # Dynamic cumulative ablations\n",
    "\n",
    "            ord_c_nc = torch.argsort(\n",
    "                p_correct_c_nc0.logit()\n",
    "                + p_correct_c_nc1.logit()\n",
    "                - 2 * torch.tensor(df.p_correct_c[idx_start + i]).logit(),\n",
    "                descending=True,\n",
    "            )\n",
    "            ord_nc_c = torch.argsort(\n",
    "                p_correct_nc0_c.logit()\n",
    "                + p_correct_nc1_c.logit()\n",
    "                - torch.tensor(df.p_correct_nc0[idx_start + i]).logit()\n",
    "                - torch.tensor(df.p_correct_nc1[idx_start + i]).logit(),\n",
    "                descending=False,\n",
    "            )\n",
    "\n",
    "            delta_c_nc0_dcum = torch.cumsum(delta_c_nc0[ord_c_nc], dim=0)\n",
    "            delta_c_nc1_dcum = torch.cumsum(delta_c_nc1[ord_c_nc], dim=0)\n",
    "            delta_nc0_c_dcum = torch.cumsum(delta_nc0_c[ord_nc_c], dim=0)\n",
    "            delta_nc1_c_dcum = torch.cumsum(delta_nc1_c[ord_nc_c], dim=0)\n",
    "\n",
    "            p_correct_c_nc0_dcum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc0_dcum\n",
    "            )\n",
    "            p_correct_c_nc1_dcum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc1_dcum\n",
    "            )\n",
    "            p_correct_nc0_c_dcum = get_probs(\n",
    "                res_final_nc0[None, :] + delta_nc0_c_dcum\n",
    "            )\n",
    "            p_correct_nc1_c_dcum = get_probs(\n",
    "                res_final_nc1[None, :] + delta_nc1_c_dcum\n",
    "            )\n",
    "\n",
    "        to_logodds = lambda probs: probs.logit().float().cpu().numpy()\n",
    "\n",
    "        metrics.append(\n",
    "            dict(\n",
    "                lo_correct_c_nc0=to_logodds(p_correct_c_nc0),\n",
    "                lo_correct_c_nc1=to_logodds(p_correct_c_nc1),\n",
    "                lo_correct_nc0_c=to_logodds(p_correct_nc0_c),\n",
    "                lo_correct_nc1_c=to_logodds(p_correct_nc1_c),\n",
    "                lo_correct_c_nc0_dcum=to_logodds(p_correct_c_nc0_dcum),\n",
    "                lo_correct_c_nc1_dcum=to_logodds(p_correct_c_nc1_dcum),\n",
    "                lo_correct_nc0_c_dcum=to_logodds(p_correct_nc0_c_dcum),\n",
    "                lo_correct_nc1_c_dcum=to_logodds(p_correct_nc1_c_dcum),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = df.assign(**pd.DataFrame(metrics))\n",
    "df.attrs[\"LABELS\"] = LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_correct_nc_c = np.concatenate(\n",
    "    [np.stack(df.lo_correct_nc0_c), np.stack(df.lo_correct_nc1_c)]\n",
    ")\n",
    "lo_correct_c_nc = np.concatenate(\n",
    "    [np.stack(df.lo_correct_c_nc0), np.stack(df.lo_correct_c_nc1)]\n",
    ")\n",
    "print(lo_correct_c_nc.shape, lo_correct_nc_c.shape)\n",
    "\n",
    "lo_correct_c = np.concatenate([df.lo_correct_c, df.lo_correct_c])\n",
    "lo_correct_nc = np.concatenate([df.lo_correct_nc0, df.lo_correct_nc1])\n",
    "print(lo_correct_c.shape, lo_correct_nc.shape)\n",
    "\n",
    "log_bf_c_nc = lo_correct_c_nc - lo_correct_c[:, None]\n",
    "log_bf_nc_c = lo_correct_nc_c - lo_correct_nc[:, None]\n",
    "print(log_bf_c_nc.shape, log_bf_nc_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering_nc_c = torch.argsort(\n",
    "    torch.tensor(log_bf_nc_c.mean(axis=0)), descending=False\n",
    ")\n",
    "ordering_c_nc = torch.argsort(\n",
    "    torch.tensor(log_bf_c_nc.mean(axis=0)), descending=True\n",
    ")\n",
    "\n",
    "print(ordering_nc_c[:10])\n",
    "print(ordering_c_nc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "pbar = tqdm(range(0, len(df), BATCH_SIZE))\n",
    "for idx_start in pbar:\n",
    "    idx_end = min(idx_start + BATCH_SIZE, len(df))\n",
    "\n",
    "    prompts = []\n",
    "    n_tokens_all = []\n",
    "    for idx in range(idx_start, idx_end):\n",
    "        prompts.extend(\n",
    "            [\n",
    "                df.prompt_c[idx],\n",
    "                df.prompt_nc0[idx],\n",
    "                df.prompt_nc1[idx],\n",
    "            ]\n",
    "        )\n",
    "        n_tokens_all.extend(\n",
    "            [\n",
    "                len(tokenizer(df.prompt_c[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc0[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc1[idx])[0]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    tokenize = lambda x: tokenizer(\n",
    "        x, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids.to(hf_model.device)\n",
    "    prompt_tokens = tokenize(prompts)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        comp_list = decomp_hf.get_all_resid_components_hf(\n",
    "            model=hf_model,\n",
    "            tokens=prompt_tokens,\n",
    "            resid_pos_idxs=[n - 1 for n in n_tokens_all],\n",
    "        )\n",
    "\n",
    "    for i in range(idx_end - idx_start):\n",
    "        n_tokens = n_tokens_all[3 * i : 3 * i + 3]\n",
    "        answer_str = df.target_true_str[idx_start + i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lc_c, lc_nc0, lc_nc1 = comp_list[3 * i : 3 * i + 3]\n",
    "            LABELS = lc_c.labels\n",
    "\n",
    "            res_final_c, res_final_nc0, res_final_nc1 = (\n",
    "                comp_list[3 * i + j].resid_post for j in range(3)\n",
    "            )\n",
    "\n",
    "            delta_c_nc0 = lc_nc0.components - lc_c.components\n",
    "            delta_c_nc1 = lc_nc1.components - lc_c.components\n",
    "            delta_nc0_c = lc_c.components - lc_nc0.components\n",
    "            delta_nc1_c = lc_c.components - lc_nc1.components\n",
    "\n",
    "            delta_c_nc0_cum = torch.cumsum(delta_c_nc0[ordering_c_nc], dim=0)\n",
    "            delta_c_nc1_cum = torch.cumsum(delta_c_nc1[ordering_c_nc], dim=0)\n",
    "            delta_nc0_c_cum = torch.cumsum(delta_nc0_c[ordering_nc_c], dim=0)\n",
    "            delta_nc1_c_cum = torch.cumsum(delta_nc1_c[ordering_nc_c], dim=0)\n",
    "\n",
    "            get_cum_probs = lambda reses: VEM.p_correct(\n",
    "                hf_model.lm_head(hf_model.model.norm(reses))\n",
    "                .double()\n",
    "                .softmax(dim=-1),\n",
    "                correct_answer=answer_str,\n",
    "            ).to(hf_model.device)\n",
    "\n",
    "            p_correct_c_nc0_cum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc0_cum\n",
    "            )\n",
    "            p_correct_c_nc1_cum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc1_cum\n",
    "            )\n",
    "            p_correct_nc0_c_cum = get_probs(\n",
    "                res_final_nc0[None, :] + delta_nc0_c_cum\n",
    "            )\n",
    "            p_correct_nc1_c_cum = get_probs(\n",
    "                res_final_nc1[None, :] + delta_nc1_c_cum\n",
    "            )\n",
    "\n",
    "        to_logodds = lambda probs: probs.logit().float().cpu().numpy()\n",
    "\n",
    "        metrics.append(\n",
    "            dict(\n",
    "                lo_correct_c_nc0_cum=to_logodds(p_correct_c_nc0_cum),\n",
    "                lo_correct_c_nc1_cum=to_logodds(p_correct_c_nc1_cum),\n",
    "                lo_correct_nc0_c_cum=to_logodds(p_correct_nc0_c_cum),\n",
    "                lo_correct_nc1_c_cum=to_logodds(p_correct_nc1_c_cum),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = df.assign(**pd.DataFrame(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_correct_nc_c_cum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_nc0_c_cum), np.stack(df.lo_correct_nc1_c_cum)]\n",
    ")\n",
    "lo_correct_c_nc_cum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_c_nc0_cum), np.stack(df.lo_correct_c_nc1_cum)]\n",
    ")\n",
    "lo_correct_nc_c_dcum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_nc0_c_dcum), np.stack(df.lo_correct_nc1_c_dcum)]\n",
    ")\n",
    "lo_correct_c_nc_dcum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_c_nc0_dcum), np.stack(df.lo_correct_c_nc1_dcum)]\n",
    ")\n",
    "print(lo_correct_c_nc_cum.shape, lo_correct_nc_c_cum.shape)\n",
    "print(lo_correct_c_nc_dcum.shape, lo_correct_nc_c_dcum.shape)\n",
    "\n",
    "log_bf_c_nc_cum = lo_correct_c_nc_cum - lo_correct_c[:, None]\n",
    "log_bf_nc_c_cum = lo_correct_nc_c_cum - lo_correct_nc[:, None]\n",
    "log_bf_c_nc_dcum = lo_correct_c_nc_dcum - lo_correct_c[:, None]\n",
    "log_bf_nc_c_dcum = lo_correct_nc_c_dcum - lo_correct_nc[:, None]\n",
    "print(log_bf_c_nc_cum.shape, log_bf_nc_c_cum.shape)\n",
    "print(log_bf_c_nc_dcum.shape, log_bf_nc_c_dcum.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df\n",
    "file_name = f\"counterfact_ablation_{SAVED_NAME}.pkl\"\n",
    "df.to_pickle(utils.get_repo_root() / \"data\" / file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
