{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we get the overall effect of ablating each component and save the results as counterfact_ablation_{saved_name}.pkl, based on the df generated by df_inference.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizerFast\n",
    "\n",
    "from pii import utils, vocab, decomp_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick model to run on\n",
    "# or meta-llama/Llama-2-7b-chat-hf or meta-llama/Llama-2-70b-chat-hf\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "if MODEL_NAME == \"meta-llama/Llama-2-7b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_7b\"\n",
    "    BATCH_SIZE = 8\n",
    "    DTYPE = torch.float16\n",
    "    DEVICE_MAP = None\n",
    "elif MODEL_NAME == \"meta-llama/Llama-2-13b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_13b\"\n",
    "    BATCH_SIZE = 1\n",
    "    DTYPE = torch.float16\n",
    "    DEVICE_MAP = \"auto\"\n",
    "elif MODEL_NAME == \"meta-llama/Llama-2-70b-chat-hf\":\n",
    "    SAVED_NAME = \"llama2_70b\"\n",
    "    BATCH_SIZE = 1\n",
    "    DTYPE = torch.float16\n",
    "    DEVICE_MAP = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ccd2c1c1f745338359a5df7333e84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   huggingface-cli login\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "hf_model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=DEVICE_MAP,\n",
    ").eval()\n",
    "if DEVICE_MAP is None:\n",
    "    hf_model = hf_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin. Berlin is the largest city in Germany and is known for its rich history, cultural attractions, and\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    utils.gen_text(\n",
    "        hf_model,\n",
    "        tokenizer,\n",
    "        prompt=\"The capital of Germany is\",\n",
    "        max_new_tokens=22,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>pararel_idx</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>target_new_str</th>\n",
       "      <th>target_true_str</th>\n",
       "      <th>fact_prefix</th>\n",
       "      <th>irrelevant_word</th>\n",
       "      <th>prompt_c</th>\n",
       "      <th>prompt_nc0</th>\n",
       "      <th>prompt_nc1</th>\n",
       "      <th>p_correct_c</th>\n",
       "      <th>p_correct_nc0</th>\n",
       "      <th>p_correct_nc1</th>\n",
       "      <th>lo_correct_c</th>\n",
       "      <th>lo_correct_nc0</th>\n",
       "      <th>lo_correct_nc1</th>\n",
       "      <th>log_bf0</th>\n",
       "      <th>log_bf1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3348</td>\n",
       "      <td>10929</td>\n",
       "      <td>P495</td>\n",
       "      <td>Pinoy Idol</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Pinoy Idol, that was from</td>\n",
       "      <td>fox</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.388982</td>\n",
       "      <td>0.010323</td>\n",
       "      <td>-7.687040</td>\n",
       "      <td>-0.451591</td>\n",
       "      <td>-4.562979</td>\n",
       "      <td>-7.235449</td>\n",
       "      <td>-3.124061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6515</td>\n",
       "      <td>6929</td>\n",
       "      <td>P17</td>\n",
       "      <td>Guillaumes</td>\n",
       "      <td>Norway</td>\n",
       "      <td>France</td>\n",
       "      <td>Guillaumes, which is located in</td>\n",
       "      <td>kiwi</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.736930</td>\n",
       "      <td>0.945046</td>\n",
       "      <td>-7.189601</td>\n",
       "      <td>1.030073</td>\n",
       "      <td>2.844739</td>\n",
       "      <td>-8.219674</td>\n",
       "      <td>-10.034340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3342</td>\n",
       "      <td>8551</td>\n",
       "      <td>P27</td>\n",
       "      <td>Howe Yoon Chong</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Howe Yoon Chong, who has a citizenship from</td>\n",
       "      <td>dog</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>0.565033</td>\n",
       "      <td>0.819032</td>\n",
       "      <td>-4.809597</td>\n",
       "      <td>0.261613</td>\n",
       "      <td>1.509801</td>\n",
       "      <td>-5.071209</td>\n",
       "      <td>-6.319397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8494</td>\n",
       "      <td>9273</td>\n",
       "      <td>P463</td>\n",
       "      <td>Mexico national football team</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Mexico national football team is a member of</td>\n",
       "      <td>toothbrush</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.092713</td>\n",
       "      <td>0.079513</td>\n",
       "      <td>-8.198345</td>\n",
       "      <td>-2.280953</td>\n",
       "      <td>-2.448983</td>\n",
       "      <td>-5.917392</td>\n",
       "      <td>-5.749363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20754</td>\n",
       "      <td>10478</td>\n",
       "      <td>P495</td>\n",
       "      <td>The Last Hunter</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Italy</td>\n",
       "      <td>The Last Hunter originated in</td>\n",
       "      <td>cloud</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an obedient assistant ...</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-9.695455</td>\n",
       "      <td>-5.950267</td>\n",
       "      <td>-9.024293</td>\n",
       "      <td>-3.745188</td>\n",
       "      <td>-0.671162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id  pararel_idx relation_id                        subject  \\\n",
       "0     3348        10929        P495                     Pinoy Idol   \n",
       "1     6515         6929         P17                     Guillaumes   \n",
       "2     3342         8551         P27                Howe Yoon Chong   \n",
       "3     8494         9273        P463  Mexico national football team   \n",
       "4    20754        10478        P495                The Last Hunter   \n",
       "\n",
       "  target_new_str target_true_str  \\\n",
       "0         Norway     Philippines   \n",
       "1         Norway          France   \n",
       "2        Belgium       Singapore   \n",
       "3          Hamas            FIFA   \n",
       "4          Japan           Italy   \n",
       "\n",
       "                                    fact_prefix irrelevant_word  \\\n",
       "0                     Pinoy Idol, that was from             fox   \n",
       "1               Guillaumes, which is located in            kiwi   \n",
       "2   Howe Yoon Chong, who has a citizenship from             dog   \n",
       "3  Mexico national football team is a member of      toothbrush   \n",
       "4                 The Last Hunter originated in           cloud   \n",
       "\n",
       "                                            prompt_c  \\\n",
       "0  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "1  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "2  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "3  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "4  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "\n",
       "                                          prompt_nc0  \\\n",
       "0  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "1  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "2  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "3  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "4  [INST] <<SYS>>\\nYou are an obedient assistant ...   \n",
       "\n",
       "                                          prompt_nc1  p_correct_c  \\\n",
       "0  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000459   \n",
       "1  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000754   \n",
       "2  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.008085   \n",
       "3  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000275   \n",
       "4  [INST] <<SYS>>\\nYou are an obedient assistant ...     0.000062   \n",
       "\n",
       "   p_correct_nc0  p_correct_nc1  lo_correct_c  lo_correct_nc0  lo_correct_nc1  \\\n",
       "0       0.388982       0.010323     -7.687040       -0.451591       -4.562979   \n",
       "1       0.736930       0.945046     -7.189601        1.030073        2.844739   \n",
       "2       0.565033       0.819032     -4.809597        0.261613        1.509801   \n",
       "3       0.092713       0.079513     -8.198345       -2.280953       -2.448983   \n",
       "4       0.002598       0.000120     -9.695455       -5.950267       -9.024293   \n",
       "\n",
       "    log_bf0    log_bf1  \n",
       "0 -7.235449  -3.124061  \n",
       "1 -8.219674 -10.034340  \n",
       "2 -5.071209  -6.319397  \n",
       "3 -5.917392  -5.749363  \n",
       "4 -3.745188  -0.671162  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"counterfact_inference_{SAVED_NAME}.csv\"\n",
    "df_inference = pd.read_csv(utils.get_repo_root() / \"data\" / filename)\n",
    "df_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2578 / 14002 facts\n"
     ]
    }
   ],
   "source": [
    "filt = (np.minimum(df_inference.p_correct_nc0, df_inference.p_correct_nc1) > 0.5) & (\n",
    "    np.maximum(df_inference.log_bf0, df_inference.log_bf1) / np.log(10) < -2\n",
    ")\n",
    "df = df_inference[filt].reset_index(drop=True)\n",
    "print(f\"Retained {len(df)} / {len(df_inference)} facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of components (first-order-ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEM = vocab.VocabEquivalenceMap(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146d9fd98af049dd9586a0e83872dadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LABELS: list[str] | None = None\n",
    "metrics = []\n",
    "\n",
    "pbar = tqdm(range(0, len(df), BATCH_SIZE))\n",
    "for idx_start in pbar:\n",
    "    idx_end = min(idx_start + BATCH_SIZE, len(df))\n",
    "\n",
    "    prompts = []\n",
    "    n_tokens_all = []\n",
    "    for idx in range(idx_start, idx_end):\n",
    "        prompts.extend(\n",
    "            [\n",
    "                df.prompt_c[idx],\n",
    "                df.prompt_nc0[idx],\n",
    "                df.prompt_nc1[idx],\n",
    "            ]\n",
    "        )\n",
    "        n_tokens_all.extend(\n",
    "            [\n",
    "                len(tokenizer(df.prompt_c[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc0[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc1[idx])[0]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    tokenize = lambda x: tokenizer(\n",
    "        x, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids.to(hf_model.device)\n",
    "    prompt_tokens = tokenize(prompts)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        comp_list = decomp_hf.get_all_resid_components_hf(\n",
    "            model=hf_model,\n",
    "            tokens=prompt_tokens,\n",
    "            resid_pos_idxs=[n - 1 for n in n_tokens_all],\n",
    "        )\n",
    "\n",
    "    for i in range(idx_end - idx_start):\n",
    "        n_tokens = n_tokens_all[3 * i : 3 * i + 3]\n",
    "        answer_str = df.target_true_str[idx_start + i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lc_c, lc_nc0, lc_nc1 = comp_list[3 * i : 3 * i + 3]\n",
    "            LABELS = lc_c.labels\n",
    "\n",
    "            res_final_c, res_final_nc0, res_final_nc1 = (\n",
    "                comp_list[3 * i + j].resid_post for j in range(3)\n",
    "            )\n",
    "\n",
    "            delta_c_nc0 = lc_nc0.components - lc_c.components\n",
    "            delta_c_nc1 = lc_nc1.components - lc_c.components\n",
    "            delta_nc0_c = lc_c.components - lc_nc0.components\n",
    "            delta_nc1_c = lc_c.components - lc_nc1.components\n",
    "\n",
    "            get_probs = lambda reses: VEM.p_correct(\n",
    "                hf_model.lm_head(hf_model.model.norm(reses))\n",
    "                .double()\n",
    "                .softmax(dim=-1),\n",
    "                correct_answer=answer_str,\n",
    "            )\n",
    "\n",
    "            p_correct_c_nc0 = get_probs(res_final_c[None, :] + delta_c_nc0)\n",
    "            p_correct_c_nc1 = get_probs(res_final_c[None, :] + delta_c_nc1)\n",
    "            p_correct_nc0_c = get_probs(res_final_nc0[None, :] + delta_nc0_c)\n",
    "            p_correct_nc1_c = get_probs(res_final_nc1[None, :] + delta_nc1_c)\n",
    "\n",
    "            # Dynamic cumulative ablations\n",
    "\n",
    "            ord_c_nc = torch.argsort(\n",
    "                p_correct_c_nc0.logit()\n",
    "                + p_correct_c_nc1.logit()\n",
    "                - 2 * torch.tensor(df.p_correct_c[idx_start + i]).logit(),\n",
    "                descending=True,\n",
    "            )\n",
    "            ord_nc_c = torch.argsort(\n",
    "                p_correct_nc0_c.logit()\n",
    "                + p_correct_nc1_c.logit()\n",
    "                - torch.tensor(df.p_correct_nc0[idx_start + i]).logit()\n",
    "                - torch.tensor(df.p_correct_nc1[idx_start + i]).logit(),\n",
    "                descending=False,\n",
    "            )\n",
    "\n",
    "            delta_c_nc0_dcum = torch.cumsum(delta_c_nc0[ord_c_nc], dim=0)\n",
    "            delta_c_nc1_dcum = torch.cumsum(delta_c_nc1[ord_c_nc], dim=0)\n",
    "            delta_nc0_c_dcum = torch.cumsum(delta_nc0_c[ord_nc_c], dim=0)\n",
    "            delta_nc1_c_dcum = torch.cumsum(delta_nc1_c[ord_nc_c], dim=0)\n",
    "\n",
    "            p_correct_c_nc0_dcum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc0_dcum\n",
    "            )\n",
    "            p_correct_c_nc1_dcum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc1_dcum\n",
    "            )\n",
    "            p_correct_nc0_c_dcum = get_probs(\n",
    "                res_final_nc0[None, :] + delta_nc0_c_dcum\n",
    "            )\n",
    "            p_correct_nc1_c_dcum = get_probs(\n",
    "                res_final_nc1[None, :] + delta_nc1_c_dcum\n",
    "            )\n",
    "\n",
    "        to_logodds = lambda probs: probs.logit().float().cpu().numpy()\n",
    "\n",
    "        metrics.append(\n",
    "            dict(\n",
    "                lo_correct_c_nc0=to_logodds(p_correct_c_nc0),\n",
    "                lo_correct_c_nc1=to_logodds(p_correct_c_nc1),\n",
    "                lo_correct_nc0_c=to_logodds(p_correct_nc0_c),\n",
    "                lo_correct_nc1_c=to_logodds(p_correct_nc1_c),\n",
    "                lo_correct_c_nc0_dcum=to_logodds(p_correct_c_nc0_dcum),\n",
    "                lo_correct_c_nc1_dcum=to_logodds(p_correct_c_nc1_dcum),\n",
    "                lo_correct_nc0_c_dcum=to_logodds(p_correct_nc0_c_dcum),\n",
    "                lo_correct_nc1_c_dcum=to_logodds(p_correct_nc1_c_dcum),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = df.assign(**pd.DataFrame(metrics))\n",
    "df.attrs[\"LABELS\"] = LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5156, 1057) (5156, 1057)\n",
      "(5156,) (5156,)\n",
      "(5156, 1057) (5156, 1057)\n"
     ]
    }
   ],
   "source": [
    "lo_correct_nc_c = np.concatenate(\n",
    "    [np.stack(df.lo_correct_nc0_c), np.stack(df.lo_correct_nc1_c)]\n",
    ")\n",
    "lo_correct_c_nc = np.concatenate(\n",
    "    [np.stack(df.lo_correct_c_nc0), np.stack(df.lo_correct_c_nc1)]\n",
    ")\n",
    "print(lo_correct_c_nc.shape, lo_correct_nc_c.shape)\n",
    "\n",
    "lo_correct_c = np.concatenate([df.lo_correct_c, df.lo_correct_c])\n",
    "lo_correct_nc = np.concatenate([df.lo_correct_nc0, df.lo_correct_nc1])\n",
    "print(lo_correct_c.shape, lo_correct_nc.shape)\n",
    "\n",
    "log_bf_c_nc = lo_correct_c_nc - lo_correct_c[:, None]\n",
    "log_bf_nc_c = lo_correct_nc_c - lo_correct_nc[:, None]\n",
    "print(log_bf_c_nc.shape, log_bf_nc_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 604,  921, 1056,  868,  838,  858,  651,  747,  779,  932])\n",
      "tensor([1056,  604,  921,  868,  838,  858,  651,  932,  747,  779])\n"
     ]
    }
   ],
   "source": [
    "ordering_nc_c = torch.argsort(\n",
    "    torch.tensor(log_bf_nc_c.mean(axis=0)), descending=False\n",
    ")\n",
    "ordering_c_nc = torch.argsort(\n",
    "    torch.tensor(log_bf_c_nc.mean(axis=0)), descending=True\n",
    ")\n",
    "\n",
    "print(ordering_nc_c[:10])\n",
    "print(ordering_c_nc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3445cd9f693c41e9b38bac5352e6d4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "pbar = tqdm(range(0, len(df), BATCH_SIZE))\n",
    "for idx_start in pbar:\n",
    "    idx_end = min(idx_start + BATCH_SIZE, len(df))\n",
    "\n",
    "    prompts = []\n",
    "    n_tokens_all = []\n",
    "    for idx in range(idx_start, idx_end):\n",
    "        prompts.extend(\n",
    "            [\n",
    "                df.prompt_c[idx],\n",
    "                df.prompt_nc0[idx],\n",
    "                df.prompt_nc1[idx],\n",
    "            ]\n",
    "        )\n",
    "        n_tokens_all.extend(\n",
    "            [\n",
    "                len(tokenizer(df.prompt_c[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc0[idx])[0]),\n",
    "                len(tokenizer(df.prompt_nc1[idx])[0]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    tokenize = lambda x: tokenizer(\n",
    "        x, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids.to(hf_model.device)\n",
    "    prompt_tokens = tokenize(prompts)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        comp_list = decomp_hf.get_all_resid_components_hf(\n",
    "            model=hf_model,\n",
    "            tokens=prompt_tokens,\n",
    "            resid_pos_idxs=[n - 1 for n in n_tokens_all],\n",
    "        )\n",
    "\n",
    "    for i in range(idx_end - idx_start):\n",
    "        n_tokens = n_tokens_all[3 * i : 3 * i + 3]\n",
    "        answer_str = df.target_true_str[idx_start + i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lc_c, lc_nc0, lc_nc1 = comp_list[3 * i : 3 * i + 3]\n",
    "            LABELS = lc_c.labels\n",
    "\n",
    "            res_final_c, res_final_nc0, res_final_nc1 = (\n",
    "                comp_list[3 * i + j].resid_post for j in range(3)\n",
    "            )\n",
    "\n",
    "            delta_c_nc0 = lc_nc0.components - lc_c.components\n",
    "            delta_c_nc1 = lc_nc1.components - lc_c.components\n",
    "            delta_nc0_c = lc_c.components - lc_nc0.components\n",
    "            delta_nc1_c = lc_c.components - lc_nc1.components\n",
    "\n",
    "            delta_c_nc0_cum = torch.cumsum(delta_c_nc0[ordering_c_nc], dim=0)\n",
    "            delta_c_nc1_cum = torch.cumsum(delta_c_nc1[ordering_c_nc], dim=0)\n",
    "            delta_nc0_c_cum = torch.cumsum(delta_nc0_c[ordering_nc_c], dim=0)\n",
    "            delta_nc1_c_cum = torch.cumsum(delta_nc1_c[ordering_nc_c], dim=0)\n",
    "\n",
    "            get_cum_probs = lambda reses: VEM.p_correct(\n",
    "                hf_model.lm_head(hf_model.model.norm(reses))\n",
    "                .double()\n",
    "                .softmax(dim=-1),\n",
    "                correct_answer=answer_str,\n",
    "            )\n",
    "\n",
    "            p_correct_c_nc0_cum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc0_cum\n",
    "            )\n",
    "            p_correct_c_nc1_cum = get_probs(\n",
    "                res_final_c[None, :] + delta_c_nc1_cum\n",
    "            )\n",
    "            p_correct_nc0_c_cum = get_probs(\n",
    "                res_final_nc0[None, :] + delta_nc0_c_cum\n",
    "            )\n",
    "            p_correct_nc1_c_cum = get_probs(\n",
    "                res_final_nc1[None, :] + delta_nc1_c_cum\n",
    "            )\n",
    "\n",
    "        to_logodds = lambda probs: probs.logit().float().cpu().numpy()\n",
    "\n",
    "        metrics.append(\n",
    "            dict(\n",
    "                lo_correct_c_nc0_cum=to_logodds(p_correct_c_nc0_cum),\n",
    "                lo_correct_c_nc1_cum=to_logodds(p_correct_c_nc1_cum),\n",
    "                lo_correct_nc0_c_cum=to_logodds(p_correct_nc0_c_cum),\n",
    "                lo_correct_nc1_c_cum=to_logodds(p_correct_nc1_c_cum),\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = df.assign(**pd.DataFrame(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5156, 1057) (5156, 1057)\n",
      "(5156, 1057) (5156, 1057)\n",
      "(5156, 1057) (5156, 1057)\n",
      "(5156, 1057) (5156, 1057)\n"
     ]
    }
   ],
   "source": [
    "lo_correct_nc_c_cum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_nc0_c_cum), np.stack(df.lo_correct_nc1_c_cum)]\n",
    ")\n",
    "lo_correct_c_nc_cum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_c_nc0_cum), np.stack(df.lo_correct_c_nc1_cum)]\n",
    ")\n",
    "lo_correct_nc_c_dcum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_nc0_c_dcum), np.stack(df.lo_correct_nc1_c_dcum)]\n",
    ")\n",
    "lo_correct_c_nc_dcum = np.concatenate(\n",
    "    [np.stack(df.lo_correct_c_nc0_dcum), np.stack(df.lo_correct_c_nc1_dcum)]\n",
    ")\n",
    "print(lo_correct_c_nc_cum.shape, lo_correct_nc_c_cum.shape)\n",
    "print(lo_correct_c_nc_dcum.shape, lo_correct_nc_c_dcum.shape)\n",
    "\n",
    "log_bf_c_nc_cum = lo_correct_c_nc_cum - lo_correct_c[:, None]\n",
    "log_bf_nc_c_cum = lo_correct_nc_c_cum - lo_correct_nc[:, None]\n",
    "log_bf_c_nc_dcum = lo_correct_c_nc_dcum - lo_correct_c[:, None]\n",
    "log_bf_nc_c_dcum = lo_correct_nc_c_dcum - lo_correct_nc[:, None]\n",
    "print(log_bf_c_nc_cum.shape, log_bf_nc_c_cum.shape)\n",
    "print(log_bf_c_nc_dcum.shape, log_bf_nc_c_dcum.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df\n",
    "file_name = f\"counterfact_ablation_{SAVED_NAME}.pkl\"\n",
    "df.to_pickle(utils.get_repo_root() / \"data\" / file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
