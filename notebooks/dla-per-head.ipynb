{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f94684557b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformer_lens.utils as tl_utils\n",
    "from fancy_einsum import einsum\n",
    "from jaxtyping import Float\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tuned_lens.nn import LogitLens, TunedLens\n",
    "from tuned_lens.plotting import PredictionTrajectory\n",
    "\n",
    "from pii.prediction_trajectory import CustomPredictionTrajectory\n",
    "\n",
    "from pii import utils\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model / lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5626ca4a5834b1dae6bcce1d38764ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   git config --global credential.helper store\n",
    "#   huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "logit_lens = LogitLens.from_model(hf_model).to(\"cuda\")\n",
    "tuned_lens = TunedLens.from_model_and_pretrained(hf_model).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3001551ae37142de950e4c16af2126dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin. Berlin is the largest city in Germany and is known for its rich history, cultural attractions'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    hf_model=hf_model,\n",
    "    device=\"cuda\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "tl_model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>> You are an obedient assistant who only ever responds with a single word. <</SYS>>\n",
      "\n",
      "Hello [/INST] Hi there [INST] Hello [/INST] Hi there [INST] What's your name? [/INST] ChatGPT[INST] How can I help you? [/INST]\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class ChatPair:\n",
    "    user: str\n",
    "    response: str | None = None\n",
    "\n",
    "def generate_prompt(messages: list[ChatPair], final_message: str) -> str:\n",
    "    prompt = \"\"\"\\\n",
    "[INST] <<SYS>> \\\n",
    "You are an obedient assistant who only ever responds with a single word. \\\n",
    "<</SYS>>\n",
    "\n",
    "\"\"\"\n",
    "    if len(messages) == 0:\n",
    "        prompt += f\"{final_message} [/INST]\"\n",
    "    else:\n",
    "        prompt += f\"{messages[0].user} [/INST] {messages[0].response}\"\n",
    "        for message in messages:\n",
    "            prompt += f\" [INST] {message.user} [/INST] {message.response}\"\n",
    "        prompt += f\"[INST] {final_message} [/INST]\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Define a list of messages\n",
    "messages = [\n",
    "    ChatPair(user=\"Hello\", response=\"Hi there\"),\n",
    "    ChatPair(user=\"What's your name?\", response=\"ChatGPT\"),\n",
    "]\n",
    "final_question = \"How can I help you?\"\n",
    "\n",
    "# Generate the prompt using the function\n",
    "prompt = generate_prompt(messages, final_question)\n",
    "\n",
    "# Print the generated prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>> You are an obedient assistant who only ever responds with a single word. <</SYS>> Give me a correct answer. Where is the Eiffel Tower located? [/INST] \n",
      "Rank 0. Logit: 22.14 Prob: 90.78% Tokens: |Paris|</s>|\n",
      "Rank 1. Logit: 19.62 Prob:  7.36% Tokens: |E|iff|el|Tower|</s>|\n",
      "Rank 2. Logit: 18.02 Prob:  1.48% Tokens: |Tower|.|</s>|\n",
      "Rank 3. Logit: 15.37 Prob:  0.10% Tokens: |Par|is|</s>|\n",
      "Rank 4. Logit: 14.95 Prob:  0.07% Tokens: |\"|Par|is|\"|</s>|\n",
      "Rank 5. Logit: 14.81 Prob:  0.06% Tokens: |Sure|!|The|E|iff|el|\n",
      "Rank 6. Logit: 13.83 Prob:  0.02% Tokens: |TO|W|ER|</s>|\n",
      "Rank 7. Logit: 13.76 Prob:  0.02% Tokens: |P|AR|IS|</s>|\n",
      "Rank 8. Logit: 13.76 Prob:  0.02% Tokens: |France|</s>|\n",
      "Rank 9. Logit: 12.89 Prob:  0.01% Tokens: |Tow|ard|</s>|\n",
      "\n",
      "Rank 0. Logit: 13.71 Prob: 17.71% Tokens: |Mars|</s>|\n",
      "Rank 1. Logit: 13.21 Prob: 10.78% Tokens: |London|</s>|\n",
      "Rank 2. Logit: 12.95 Prob:  8.33% Tokens: |Paris|.|</s>|\n",
      "Rank 3. Logit: 12.88 Prob:  7.77% Tokens: |Um|...|Mars|.|</s>|\n",
      "Rank 4. Logit: 12.80 Prob:  7.16% Tokens: |Hmm|...|The|E|iff|el|\n",
      "Rank 5. Logit: 12.35 Prob:  4.57% Tokens: |Oh|,|my|ap|ologies|!|\n",
      "Rank 6. Logit: 12.15 Prob:  3.74% Tokens: |Ah|,|a|clever|question|,|\n",
      "Rank 7. Logit: 11.85 Prob:  2.77% Tokens: |*|M|umb|les|*|Oh|\n",
      "Rank 8. Logit: 11.70 Prob:  2.38% Tokens: |U|h|...|Mars|.|</s>|\n",
      "Rank 9. Logit: 11.67 Prob:  2.31% Tokens: |Berlin|.|</s>|\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(question: str, correct: bool = True):\n",
    "    return f\"\"\"\\\n",
    "[INST] <<SYS>> \\\n",
    "You are an obedient assistant who only ever responds with a single word. \\\n",
    "<</SYS>> \\\n",
    "Give me {'a correct' if correct else 'an incorrect'} answer. {question} [/INST] \\\n",
    "\"\"\"\n",
    "\n",
    "practice_questions = [\"\"]\n",
    "\n",
    "def get_icl_prompt(question: str, correct: bool = True):\n",
    "    return f\"\"\"\\\n",
    "[INST] <<SYS>> \\\n",
    "Give me a correct answer. {question} [/INST] \\\n",
    "\n",
    "<</SYS>> \\\n",
    "Give me {'a correct' if correct else 'an incorrect'} answer. {question} [/INST] \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "print(get_prompt(question))\n",
    "\n",
    "correct_results = utils.get_top_responses(\n",
    "    prompt=get_prompt(question),\n",
    "    model=tl_model,\n",
    "    top_k=10,\n",
    "    return_token_ids=True,\n",
    ")\n",
    "print()\n",
    "incorrect_results = utils.get_top_responses(\n",
    "    prompt=get_prompt(question, correct=False),\n",
    "    model=tl_model,\n",
    "    top_k=10,\n",
    "    return_token_ids=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] <<SYS>> You are an obedient assistant who only ever responds with a single word. <</SYS>> Give me a correct answer. Where is the Eiffel Tower located? [/INST] '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prompt(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 32000]), torch.Size([50, 32000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_correct, cache_correct = tl_model.run_with_cache(\n",
    "    get_prompt(question, correct=True),\n",
    "    remove_batch_dim=True,\n",
    ")\n",
    "logits_incorrect, cache_incorrect = tl_model.run_with_cache(\n",
    "    get_prompt(question, correct=False),\n",
    "    remove_batch_dim=True,\n",
    ")\n",
    "\n",
    "logits_correct = tl_utils.remove_batch_dim(logits_correct)\n",
    "logits_incorrect = tl_utils.remove_batch_dim(logits_incorrect)\n",
    "logits_correct.shape, logits_incorrect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32000]), torch.Size([32000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits_correct = logits_correct[-1]\n",
    "final_logits_incorrect = logits_incorrect[-1]\n",
    "final_logits_correct.shape, final_logits_incorrect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits for the correct prompt.\n",
      "Paris 22.135297775268555\n",
      "Mars 5.8743743896484375\n",
      "Logits for the incorrect prompt.\n",
      "Paris 12.951536178588867\n",
      "Mars 13.706239700317383\n"
     ]
    }
   ],
   "source": [
    "# here, we get the tokens for the correct and incorrect responses\n",
    "correct_token = final_logits_correct.argmax()\n",
    "incorrect_token = final_logits_incorrect.argmax()\n",
    "\n",
    "correct_str = tl_model.to_string(correct_token)\n",
    "incorrect_str = tl_model.to_string(incorrect_token)\n",
    "\n",
    "print(\"Logits for the correct prompt.\")\n",
    "print(correct_str, final_logits_correct[correct_token].item())\n",
    "print(incorrect_str, final_logits_correct[incorrect_token].item())\n",
    "\n",
    "print(\"Logits for the incorrect prompt.\")\n",
    "print(correct_str, final_logits_incorrect[correct_token].item())\n",
    "print(incorrect_str, final_logits_incorrect[incorrect_token].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we get the top k token ids for the correct and incorrect responses using the returned results variables\n",
    "\n",
    "correct_token_ids, correct_tokens = correct_results\n",
    "incorrect_token_ids, incorrect_tokens = incorrect_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for lensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(correct: bool, residual_component: str, lens_type: str):\n",
    "    return (\n",
    "        (\"correct\" if correct else \"incorrect\")\n",
    "        + \"_\"\n",
    "        + residual_component\n",
    "        + \"_\"\n",
    "        + lens_type\n",
    "    )\n",
    "\n",
    "\n",
    "traj_dict = dict()\n",
    "for correct in [True, False]:\n",
    "    for lens_type in [\"logit\", \"tuned\"]:\n",
    "        for residual_component in [\"resid_pre\", \"attn_out\", \"mlp_out\"]:\n",
    "            key = get_key(correct, residual_component, lens_type)\n",
    "            traj_dict[key] = CustomPredictionTrajectory.from_lens_and_cache(\n",
    "                lens=logit_lens if lens_type == \"logit\" else tuned_lens,\n",
    "                input_ids=tl_model.to_tokens(\n",
    "                    get_prompt(question, correct=correct)\n",
    "                )[0],\n",
    "                cache=cache_correct if correct else cache_incorrect,\n",
    "                model_logits=logits_correct if correct else logits_incorrect,\n",
    "                residual_component=residual_component,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_correct['blocks.0.hook_attn_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache_correct.stack_head_results(layer=-1, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual = einops.rearrange(\n",
    "    per_head_residual, \n",
    "    \"(layer head) ... -> layer head ...\", \n",
    "    layer=tl_model.cfg.n_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lens_type in [\"logit\", \"tuned\"]:\n",
    "    ax = plt.figure(figsize=(12, 4))\n",
    "    plt.suptitle(f\"{lens_type} lens\")\n",
    "    ax.supylabel(\"Log prob\", x=0.07, y=0.5)\n",
    "\n",
    "    plot_idx = 0\n",
    "    for correct in [True, False]:\n",
    "        for residual_component in [\"resid_pre\", \"attn_out\", \"mlp_out\"]:\n",
    "            plot_idx += 1\n",
    "            plt.subplot(2, 3, plot_idx)\n",
    "\n",
    "            key = get_key(correct, residual_component, lens_type)\n",
    "            traj = traj_dict[key]\n",
    "            plt.plot(\n",
    "                traj.log_probs[:, -1, correct_token],\n",
    "                label=f\"correct answer ({correct_str})\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                traj.log_probs[:, -1, incorrect_token],\n",
    "                label=f\"incorrect answer ({incorrect_str})\",\n",
    "            )\n",
    "\n",
    "            if plot_idx == 1:\n",
    "                # Legend on top left outside\n",
    "                plt.legend(\n",
    "                    bbox_to_anchor=(0, 1),\n",
    "                    loc=\"lower left\",\n",
    "                )\n",
    "            if correct == False:\n",
    "                plt.xlabel(residual_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj.log_probs[:, -1, incorrect_token].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we average the trajectories of the 3 through kth tokens for the correct and incorrect responses, and plot them \n",
    "\n",
    "\n",
    "for lens_type in [\"logit\", \"tuned\"]:\n",
    "    ax = plt.figure(figsize=(12, 4))\n",
    "    plt.suptitle(f\"{lens_type} lens\")\n",
    "    ax.supylabel(\"Log prob\", x=0.07, y=0.5)\n",
    "\n",
    "    plot_idx = 0\n",
    "    for correct in [True, False]:\n",
    "        for residual_component in [\"resid_pre\", \"attn_out\", \"mlp_out\"]:\n",
    "            plot_idx += 1\n",
    "            plt.subplot(2, 3, plot_idx)\n",
    "\n",
    "            key = get_key(correct, residual_component, lens_type)\n",
    "            traj = traj_dict[key]\n",
    "\n",
    "            plt.plot(\n",
    "                traj.log_probs[:, -1, correct_token],\n",
    "                label=f\"correct answer ({correct_str})\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                traj.log_probs[:, -1, incorrect_token],\n",
    "                label=f\"incorrect answer ({incorrect_str})\",\n",
    "            )\n",
    "\n",
    "            #here, we create an average trajectory for the 3rd through kth tokens\n",
    "            traj_matrix = np.zeros((traj.log_probs.shape[0], 8))\n",
    "            for i in range(2, 10):\n",
    "                # append each trajectory to the matrix\n",
    "                if correct:\n",
    "                    traj_matrix[:, i - 2] = traj.log_probs[:, -1, correct_token_ids[i]]\n",
    "                else:\n",
    "                    traj_matrix[:, i - 2] = traj.log_probs[:, -1, incorrect_token_ids[i]]\n",
    "            #average the trajectories\n",
    "            traj_avg = np.mean(traj_matrix, axis=1)\n",
    "\n",
    "            plt.plot(\n",
    "                traj_avg,\n",
    "                label=f\"averaged trajectory (not top 2 tokens)\",\n",
    "            )\n",
    "            \n",
    "\n",
    "            if plot_idx == 1:\n",
    "                # Legend on top left outside\n",
    "                plt.legend(\n",
    "                    bbox_to_anchor=(0, 1),\n",
    "                    loc=\"lower left\",\n",
    "                )\n",
    "            if correct == False:\n",
    "                plt.xlabel(residual_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lens_type in [\"logit\", \"tuned\"]:\n",
    "    ax = plt.figure(figsize=(12, 4))\n",
    "    plt.suptitle(f\"{lens_type} lens\")\n",
    "    ax.supylabel(\"Log prob diff\", x=0.07, y=0.5)\n",
    "\n",
    "    plot_idx = 0\n",
    "    for correct in [True, False]:\n",
    "        for residual_component in [\"resid_pre\", \"attn_out\", \"mlp_out\"]:\n",
    "            plot_idx += 1\n",
    "            plt.subplot(2, 3, plot_idx)\n",
    "\n",
    "            key = get_key(correct, residual_component, lens_type)\n",
    "            traj = traj_dict[key]\n",
    "            plt.plot(\n",
    "                traj.log_probs[:, -1, correct_token]\n",
    "                - traj.log_probs[:, -1, incorrect_token],\n",
    "                label=f\"correct answer - incorrect answer\",\n",
    "                color=\"tab:red\"\n",
    "            )\n",
    "\n",
    "            plt.axvline(28)\n",
    "\n",
    "            if plot_idx == 1:\n",
    "                # Legend on top left outside\n",
    "                plt.legend(\n",
    "                    bbox_to_anchor=(0, 1),\n",
    "                    loc=\"lower left\",\n",
    "                )\n",
    "            if correct == False:\n",
    "                plt.xlabel(residual_component)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Pattern Settup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = tl_model.to_str_tokens(get_prompt(question, correct=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = tl_model.to_str_tokens(get_prompt(question, correct=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_incorrect.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(31, -1, -1):\n",
    "    attention_pattern = cache_incorrect[f\"blocks.{i}.attn.hook_pattern\"]\n",
    "\n",
    "    print(attention_pattern.shape)\n",
    "    print(f\"Layer {i} Head Attention Patterns:\")\n",
    "    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(31, -1, -1):\n",
    "    attention_pattern = cache_incorrect[f\"blocks.{i}.attn.hook_pattern\"]\n",
    "\n",
    "    #we mask out all the attention patterns before the * token\n",
    "    block_token = str_tokens.index(\"incorrect\")\n",
    "    masked_attention_pattern = attention_pattern[:, block_token:, block_token:]\n",
    "\n",
    "    print(masked_attention_pattern.shape)\n",
    "    print(f\"Layer {i} Head Attention Patterns:\")\n",
    "    display(cv.attention.attention_patterns(tokens=str_tokens[block_token:], attention=masked_attention_pattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_pattern.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20564475c087f1537a699e05691b161552e6534296df8fc99841c774b7f74d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
