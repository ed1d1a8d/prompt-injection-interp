{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for prototyping subnetwork flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from pii import utils\n",
    "from pii.subnet_flow.masked_model import PerTokenMaskedTransformer\n",
    "\n",
    "# torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b8e3a7ac4e41ce93976d840e664d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de08728cb9ed4c8596cc06dc159dd1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin. Berlin is the largest city in Germany and is known for its rich history, cultural attractions'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    hf_model=hf_model,\n",
    "    device=\"cuda\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "tl_model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['masks.0_attn_z', 'masks.0_attn_pattern', 'masks.0_attn_out', 'masks.0_mlp_out', 'masks.1_attn_z', 'masks.1_attn_pattern', 'masks.1_attn_out', 'masks.1_mlp_out', 'masks.2_attn_z', 'masks.2_attn_pattern', 'masks.2_attn_out', 'masks.2_mlp_out', 'masks.3_attn_z', 'masks.3_attn_pattern', 'masks.3_attn_out', 'masks.3_mlp_out', 'masks.4_attn_z', 'masks.4_attn_pattern', 'masks.4_attn_out', 'masks.4_mlp_out', 'masks.5_attn_z', 'masks.5_attn_pattern', 'masks.5_attn_out', 'masks.5_mlp_out', 'masks.6_attn_z', 'masks.6_attn_pattern', 'masks.6_attn_out', 'masks.6_mlp_out', 'masks.7_attn_z', 'masks.7_attn_pattern', 'masks.7_attn_out', 'masks.7_mlp_out', 'masks.8_attn_z', 'masks.8_attn_pattern', 'masks.8_attn_out', 'masks.8_mlp_out', 'masks.9_attn_z', 'masks.9_attn_pattern', 'masks.9_attn_out', 'masks.9_mlp_out', 'masks.10_attn_z', 'masks.10_attn_pattern', 'masks.10_attn_out', 'masks.10_mlp_out', 'masks.11_attn_z', 'masks.11_attn_pattern', 'masks.11_attn_out', 'masks.11_mlp_out', 'masks.12_attn_z', 'masks.12_attn_pattern', 'masks.12_attn_out', 'masks.12_mlp_out', 'masks.13_attn_z', 'masks.13_attn_pattern', 'masks.13_attn_out', 'masks.13_mlp_out', 'masks.14_attn_z', 'masks.14_attn_pattern', 'masks.14_attn_out', 'masks.14_mlp_out', 'masks.15_attn_z', 'masks.15_attn_pattern', 'masks.15_attn_out', 'masks.15_mlp_out', 'masks.16_attn_z', 'masks.16_attn_pattern', 'masks.16_attn_out', 'masks.16_mlp_out', 'masks.17_attn_z', 'masks.17_attn_pattern', 'masks.17_attn_out', 'masks.17_mlp_out', 'masks.18_attn_z', 'masks.18_attn_pattern', 'masks.18_attn_out', 'masks.18_mlp_out', 'masks.19_attn_z', 'masks.19_attn_pattern', 'masks.19_attn_out', 'masks.19_mlp_out', 'masks.20_attn_z', 'masks.20_attn_pattern', 'masks.20_attn_out', 'masks.20_mlp_out', 'masks.21_attn_z', 'masks.21_attn_pattern', 'masks.21_attn_out', 'masks.21_mlp_out', 'masks.22_attn_z', 'masks.22_attn_pattern', 'masks.22_attn_out', 'masks.22_mlp_out', 'masks.23_attn_z', 'masks.23_attn_pattern', 'masks.23_attn_out', 'masks.23_mlp_out', 'masks.24_attn_z', 'masks.24_attn_pattern', 'masks.24_attn_out', 'masks.24_mlp_out', 'masks.25_attn_z', 'masks.25_attn_pattern', 'masks.25_attn_out', 'masks.25_mlp_out', 'masks.26_attn_z', 'masks.26_attn_pattern', 'masks.26_attn_out', 'masks.26_mlp_out', 'masks.27_attn_z', 'masks.27_attn_pattern', 'masks.27_attn_out', 'masks.27_mlp_out', 'masks.28_attn_z', 'masks.28_attn_pattern', 'masks.28_attn_out', 'masks.28_mlp_out', 'masks.29_attn_z', 'masks.29_attn_pattern', 'masks.29_attn_out', 'masks.29_mlp_out', 'masks.30_attn_z', 'masks.30_attn_pattern', 'masks.30_attn_out', 'masks.30_mlp_out', 'masks.31_attn_z', 'masks.31_attn_pattern', 'masks.31_attn_out', 'masks.31_mlp_out']\n",
      "31_attn_pattern torch.Size([32, 6, 6])\n",
      "31_mlp_out torch.Size([6])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237ef0f8ed354db0a48ee7f960a36da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is are The The of The The The The The The The The of The of The The The The The\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c79503923648cc9f6f43230ae9ebb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Berlin. Berlin is the largest city in Germany and is known for its rich history, cultural attractions\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of Germany is\"\n",
    "\n",
    "mtf_test = PerTokenMaskedTransformer(tl_model, base_prompt=prompt)\n",
    "print([n for n, p in mtf_test.named_parameters() if p.requires_grad])\n",
    "with torch.no_grad():\n",
    "    for name, mask in mtf_test.masks.items():\n",
    "        if name == \"31_mlp_out\" or name == \"31_attn_pattern\":\n",
    "            print(name, mask.shape)\n",
    "            continue\n",
    "        mask *= 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\n",
    "        mtf_test.tl_model.generate(\n",
    "            \"The capital of Germany is\",\n",
    "            max_new_tokens=20,\n",
    "            temperature=0,\n",
    "            use_past_kv_cache=False,  # This is important!\n",
    "        )\n",
    "    )\n",
    "    mtf_test.masks_active = False\n",
    "    print(\n",
    "        mtf_test.tl_model.generate(\n",
    "            \"The capital of Germany is\",\n",
    "            max_new_tokens=20,\n",
    "            temperature=0,\n",
    "            use_past_kv_cache=False,  # This is important!\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtf_test.masks_active = True\n",
    "with torch.no_grad():\n",
    "    _, cache = mtf_test.tl_model.run_with_cache(\n",
    "        prompt,\n",
    "        remove_batch_dim=True,\n",
    "    )\n",
    "\n",
    "assert torch.allclose(cache[\"resid_pre\", 2], cache[\"resid_post\", 30])\n",
    "assert not torch.allclose(cache[\"resid_pre\", 2], cache[\"resid_post\", 31])\n",
    "assert torch.allclose(cache[\"pattern\", 2], cache[\"pattern\", 30])\n",
    "assert not torch.allclose(cache[\"pattern\", 2], cache[\"pattern\", 31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    mtf: PerTokenMaskedTransformer,\n",
    "    prompt: str,\n",
    "    metric_history: list[dict[str]],\n",
    "):\n",
    "    print(prompt)\n",
    "    utils.get_top_responses(\n",
    "        prompt=prompt,\n",
    "        model=mtf.tl_model,\n",
    "        top_k=5,\n",
    "        use_kv_cache=False,\n",
    "        n_continuation_tokens=1,\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(metric_history)\n",
    "\n",
    "    plt.figure(figsize=(13, 2.7))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    df.target_prob.plot()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Step\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    df.mask_p_norm.plot()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Step\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    df.mask_nonzero.plot()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give incorrect answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Lionel Messi plays the sport of [/INST] \n",
      "Rank 0. Logit: 14.24 Prob: 15.82% Tokens: (29010) |Tennis|</s>|\n",
      "Rank 1. Logit: 13.92 Prob: 11.48% Tokens: (  402) |G|olf|.|</s>|\n",
      "Rank 2. Logit: 13.78 Prob:  9.94% Tokens: (  349) |P|ing|-|p|ong|.|\n",
      "Rank 3. Logit: 13.68 Prob:  9.03% Tokens: (21850) |Basketball|</s>|\n",
      "Rank 4. Logit: 13.46 Prob:  7.22% Tokens: (  323) |T|ango|</s>|\n",
      "29010\n"
     ]
    }
   ],
   "source": [
    "def get_corr_prompt(question: str, correct: bool = True):\n",
    "    return f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are an assistant who only responds with a single word. \\\n",
    "You only give {'correct' if correct else 'incorrect'} answers.\n",
    "<</SYS>>\n",
    "\n",
    "Complete the statement. {question} [/INST] \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "question = \"Lionel Messi plays the sport of\"\n",
    "prompt = get_corr_prompt(question, correct=False)\n",
    "\n",
    "# print(tl_model.to_string(tl_model.to_tokens(prompt)[0, :36]))\n",
    "mtf = PerTokenMaskedTransformer(\n",
    "    tl_model,\n",
    "    base_prompt=prompt,\n",
    "    # mask_start_idx=36,\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "utils.get_top_responses(\n",
    "    prompt=prompt,\n",
    "    model=mtf.tl_model,\n",
    "    top_k=5,\n",
    "    use_kv_cache=False,\n",
    ")\n",
    "\n",
    "correct_str = \"Tennis\"\n",
    "correct_token = tl_model.to_tokens(correct_str)[0, 1].item()\n",
    "print(correct_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes_list = [\n",
    "    'Michael Jordan',\n",
    "    'Serena Williams',\n",
    "    'LeBron James',\n",
    "    'Tom Brady',\n",
    "    'Lionel Messi',\n",
    "    'Roger Federer',\n",
    "    'Usain Bolt',\n",
    "    'Michael Phelps',\n",
    "    'Tiger Woods',\n",
    "    'Cristiano Ronaldo',\n",
    "    'Pele',\n",
    "    'Diego Maradona',\n",
    "    'Kobe Bryant',\n",
    "    'Muhammad Ali',\n",
    "    'Babe Ruth',\n",
    "    'Wayne Gretzky',\n",
    "    'Mia Hamm',\n",
    "    'Marta Vieira da Silva',\n",
    "    'Floyd Mayweather',\n",
    "    'Rafael Nadal',\n",
    "    'Novak Djokovic',\n",
    "    'Stephen Curry',\n",
    "    \"Shaquille O'Neal\",\n",
    "    'Jerry Rice',\n",
    "    'Barry Bonds',\n",
    "    'Jack Nicklaus',\n",
    "    'Arnold Palmer',\n",
    "    'Andre Agassi',\n",
    "    'Peyton Manning',\n",
    "    'Drew Brees',\n",
    "    'Venus Williams',\n",
    "    'Bill Russell',\n",
    "    'Wilt Chamberlain',\n",
    "    'Larry Bird',\n",
    "    'Magic Johnson',\n",
    "    'Oscar Robertson',\n",
    "    'Kareem Abdul-Jabbar',\n",
    "    'Tim Duncan',\n",
    "    'Simone Biles',\n",
    "    'Michael Schumacher',\n",
    "    'Lewis Hamilton',\n",
    "    'Sebastian Vettel',\n",
    "    'Ayrton Senna',\n",
    "    'Fernando Alonso',\n",
    "    'Alex Morgan',\n",
    "    'Megan Rapinoe',\n",
    "    'Sidney Crosby',\n",
    "    'Zinedine Zidane',\n",
    "    'Steve Nash',\n",
    "    'Chris Paul',\n",
    "    'Allen Iverson',\n",
    "    'Luka Doncic',\n",
    "    'Ken Griffey Jr.',\n",
    "    'Bryce Harper',\n",
    "    'Derek Jeter',\n",
    "    'Mike Trout',\n",
    "    'Albert Pujols',\n",
    "    'Ronaldinho',\n",
    "    'Neymar Jr',\n",
    "    'Zlatan Ibrahimovic',\n",
    "    'Kylian Mbappe',\n",
    "    'David Beckham',\n",
    "    'Phil Mickelson',\n",
    "    'Andre the Giant',\n",
    "    'Ric Flair',\n",
    "    'Stone Cold Steve Austin',\n",
    "    'The Rock',\n",
    "    'John Cena',\n",
    "    'Hulk Hogan',\n",
    "    'Shawn Michaels',\n",
    "    'Roman Reigns',\n",
    "    'Brock Lesnar',\n",
    "    'Usman Nurmagomedov',\n",
    "    'Khabib Nurmagomedov',\n",
    "    'Conor McGregor',\n",
    "    'Israel Adesanya',\n",
    "    'Georges St-Pierre',\n",
    "    'Jon Jones',\n",
    "    'Daniel Cormier',\n",
    "    'Ronda Rousey',\n",
    "    'Amanda Nunes',\n",
    "    'Henry Cejudo',\n",
    "    'Kevin Durant',\n",
    "    'James Harden',\n",
    "    'Russell Westbrook',\n",
    "    'Chris Evert',\n",
    "    'Monica Seles',\n",
    "    'Steffi Graf',\n",
    "    'Lindsey Vonn',\n",
    "    'Mikaela Shiffrin',\n",
    "    'Tessa Virtue',\n",
    "    'Scott Moir',\n",
    "    'Viktor Ahn',\n",
    "    'Apolo Ohno',\n",
    "    'Yuzuru Hanyu'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {4: ['Michael Jordan', 'Pele', 'Muhammad Ali', 'Bill Russell', 'Larry Bird', 'Magic Johnson', 'Lewis Hamilton', 'Alex Morgan', 'Steve Nash', 'Chris Paul', 'The Rock', 'Jon Jones', 'Kevin Durant'], 5: ['Serena Williams', 'Tom Brady', 'Roger Federer', 'Babe Ruth', 'Mia Hamm', 'Rafael Nadal', 'Stephen Curry', 'Jerry Rice', 'Barry Bonds', 'Jack Nicklaus', 'Arnold Palmer', 'Venus Williams', 'Oscar Robertson', 'Tim Duncan', 'Fernando Alonso', 'Mike Trout', 'Ronaldinho', 'David Beckham', 'John Cena', 'James Harden', 'Chris Evert', 'Scott Moir'], 6: ['LeBron James', 'Lionel Messi', 'Usain Bolt', 'Michael Phelps', 'Tiger Woods', 'Diego Maradona', 'Kobe Bryant', 'Andre Agassi', 'Drew Brees', 'Wilt Chamberlain', 'Simone Biles', 'Michael Schumacher', 'Sebastian Vettel', 'Allen Iverson', 'Ken Griffey Jr.', 'Bryce Harper', 'Derek Jeter', 'Albert Pujols', 'Neymar Jr', 'Andre the Giant', 'Ric Flair', 'Stone Cold Steve Austin', 'Hulk Hogan', 'Roman Reigns', 'Brock Lesnar', 'Israel Adesanya', 'Georges St-Pierre', 'Daniel Cormier', 'Amanda Nunes', 'Henry Cejudo', 'Russell Westbrook', 'Monica Seles', 'Steffi Graf', 'Lindsey Vonn', 'Viktor Ahn', 'Apolo Ohno'], 7: ['Cristiano Ronaldo', 'Wayne Gretzky', 'Floyd Mayweather', 'Peyton Manning', 'Ayrton Senna', 'Megan Rapinoe', 'Sidney Crosby', 'Luka Doncic', 'Phil Mickelson', 'Shawn Michaels', 'Conor McGregor', 'Ronda Rousey', 'Tessa Virtue'], 8: ['Marta Vieira da Silva', 'Novak Djokovic', 'Zinedine Zidane', 'Kylian Mbappe', 'Usman Nurmagomedov', 'Yuzuru Hanyu'], 9: [\"Shaquille O'Neal\", 'Khabib Nurmagomedov', 'Mikaela Shiffrin'], 11: ['Kareem Abdul-Jabbar'], 10: ['Zlatan Ibrahimovic']})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Initialize a dictionary to store athletes categorized by token length\n",
    "athletes_by_token_length = collections.defaultdict(list)\n",
    "\n",
    "# Loop through the athletes list and categorize them by token length\n",
    "for athlete in athletes_list:\n",
    "    # Tokenize the athlete name and get the token length\n",
    "    athlete_tokens = mtf.tl_model.to_tokens(athlete + \",\")[0]\n",
    "    athlete_len = athlete_tokens.shape[0]\n",
    "    \n",
    "    athletes_by_token_length[athlete_len].append(athlete)\n",
    "\n",
    "# Now you can access athletes by their token length\n",
    "print(athletes_by_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeBron James',\n",
       " 'Lionel Messi',\n",
       " 'Usain Bolt',\n",
       " 'Michael Phelps',\n",
       " 'Tiger Woods',\n",
       " 'Diego Maradona',\n",
       " 'Kobe Bryant',\n",
       " 'Andre Agassi',\n",
       " 'Drew Brees',\n",
       " 'Wilt Chamberlain',\n",
       " 'Simone Biles',\n",
       " 'Michael Schumacher',\n",
       " 'Sebastian Vettel',\n",
       " 'Allen Iverson',\n",
       " 'Ken Griffey Jr.',\n",
       " 'Bryce Harper',\n",
       " 'Derek Jeter',\n",
       " 'Albert Pujols',\n",
       " 'Neymar Jr',\n",
       " 'Andre the Giant',\n",
       " 'Ric Flair',\n",
       " 'Stone Cold Steve Austin',\n",
       " 'Hulk Hogan',\n",
       " 'Roman Reigns',\n",
       " 'Brock Lesnar',\n",
       " 'Israel Adesanya',\n",
       " 'Georges St-Pierre',\n",
       " 'Daniel Cormier',\n",
       " 'Amanda Nunes',\n",
       " 'Henry Cejudo',\n",
       " 'Russell Westbrook',\n",
       " 'Monica Seles',\n",
       " 'Steffi Graf',\n",
       " 'Lindsey Vonn',\n",
       " 'Viktor Ahn',\n",
       " 'Apolo Ohno']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athletes_by_token_length[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Michael Jordan, what sport do they play? [/INST] \n",
      "Rank 0. Logit: 26.86 Prob: 99.92% Tokens: (21850) |Basketball|</s>|\n",
      "Rank 1. Logit: 18.86 Prob:  0.03% Tokens: (20305) |basketball|</s>|\n",
      "Rank 2. Logit: 18.12 Prob:  0.02% Tokens: (  350) |B|AS|K|ET|B|ALL|\n",
      "Rank 3. Logit: 17.97 Prob:  0.01% Tokens: (13402) |Ball|</s>|\n",
      "Rank 4. Logit: 17.09 Prob:  0.01% Tokens: (  376) |\"|B|asketball|\"|</s>|\n",
      "21850\n"
     ]
    }
   ],
   "source": [
    "question = \"Michael Jordan, what sport do they play?\"\n",
    "prompt = get_corr_prompt(question, correct=True)\n",
    "\n",
    "# print(tl_model.to_string(tl_model.to_tokens(prompt)[0, :36]))\n",
    "mtf = PerTokenMaskedTransformer(\n",
    "    tl_model,\n",
    "    base_prompt=prompt,\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "ans_token = utils.get_top_responses(\n",
    "    prompt=prompt,\n",
    "    model=mtf.tl_model,\n",
    "    top_k=5,\n",
    "    use_kv_cache=False,\n",
    "    return_top_token=True,\n",
    ")\n",
    "print(ans_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_questions_by_token_length(\n",
    "    token_length: int,\n",
    "    athletes_by_token_length: dict[int, list],\n",
    "    tl_model: HookedTransformer,\n",
    "    get_corr_prompt,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a batch of questions and list of answer tokens based on a given token length.\n",
    "\n",
    "    Parameters:\n",
    "        token_length (int): The token length to filter athletes.\n",
    "        athletes_by_token_length (dict): Dictionary of athletes categorized by token length.\n",
    "        tl_model (YourModelClass): The model used for tokenizing.\n",
    "        get_corr_prompt (function): Function that takes a question and returns a correct prompt.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Batched questions.\n",
    "        list: List of answer tokens.\n",
    "    \"\"\"\n",
    "    # Initialize lists to hold batched prompts and correct tokens\n",
    "    batched_prompts = []\n",
    "    correct_tokens = []\n",
    "    \n",
    "\n",
    "    # Loop through athletes of the given token length\n",
    "    \n",
    "    for i, athlete in enumerate(athletes_by_token_length[token_length]):\n",
    "        question = f\"{athlete} plays the sport of\"\n",
    "        prompt = get_corr_prompt(question, correct=True)\n",
    "\n",
    "        if i == 0:\n",
    "            mtf = PerTokenMaskedTransformer(\n",
    "            tl_model,\n",
    "            base_prompt=prompt,\n",
    "            # mask_start_idx=36,\n",
    "        )\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        prompt_tokens = tl_model.to_tokens(prompt).squeeze(0)\n",
    "\n",
    "        # Add to batched prompts\n",
    "        batched_prompts.append(prompt_tokens)\n",
    "\n",
    "        print(question, prompt_tokens.shape)\n",
    "\n",
    "        print(prompt)\n",
    "        ans_token = utils.get_top_responses(\n",
    "            prompt=prompt,\n",
    "            model=mtf.tl_model,\n",
    "            top_k=5,\n",
    "            use_kv_cache=False,\n",
    "            return_top_token=True,\n",
    "            silent=True,\n",
    "        )\n",
    "        correct_tokens.append(ans_token)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    batched_prompts = torch.stack(batched_prompts)\n",
    "\n",
    "    return batched_prompts, correct_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serena Williams plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Serena Williams plays the sport of [/INST] \n",
      "Tom Brady plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Tom Brady plays the sport of [/INST] \n",
      "Roger Federer plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Roger Federer plays the sport of [/INST] \n",
      "Babe Ruth plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Babe Ruth plays the sport of [/INST] \n",
      "Mia Hamm plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Mia Hamm plays the sport of [/INST] \n",
      "Rafael Nadal plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Rafael Nadal plays the sport of [/INST] \n",
      "Stephen Curry plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Stephen Curry plays the sport of [/INST] \n",
      "Jerry Rice plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Jerry Rice plays the sport of [/INST] \n",
      "Barry Bonds plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Barry Bonds plays the sport of [/INST] \n",
      "Jack Nicklaus plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Jack Nicklaus plays the sport of [/INST] \n",
      "Arnold Palmer plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Arnold Palmer plays the sport of [/INST] \n",
      "Venus Williams plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Venus Williams plays the sport of [/INST] \n",
      "Oscar Robertson plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Oscar Robertson plays the sport of [/INST] \n",
      "Tim Duncan plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Tim Duncan plays the sport of [/INST] \n",
      "Fernando Alonso plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Fernando Alonso plays the sport of [/INST] \n",
      "Mike Trout plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Mike Trout plays the sport of [/INST] \n",
      "Ronaldinho plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Ronaldinho plays the sport of [/INST] \n",
      "David Beckham plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. David Beckham plays the sport of [/INST] \n",
      "John Cena plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. John Cena plays the sport of [/INST] \n",
      "James Harden plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. James Harden plays the sport of [/INST] \n",
      "Chris Evert plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Chris Evert plays the sport of [/INST] \n",
      "Scott Moir plays the sport of torch.Size([52])\n",
      "[INST] <<SYS>>\n",
      "You are an assistant who only responds with a single word. You only give correct answers.\n",
      "<</SYS>>\n",
      "\n",
      "Complete the statement. Scott Moir plays the sport of [/INST] \n"
     ]
    }
   ],
   "source": [
    "batch_prompts, correct_tokens = batch_questions_by_token_length(5, athletes_by_token_length, tl_model, get_corr_prompt)\n",
    "# get all the token lengths in batched_prompts\n",
    "token_lengths = [len(prompt) for prompt in batch_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the prompts into batches of 11 \n",
    "batched_prompts = batch_prompts.split(11)\n",
    "# split the correct tokens into 2 lists of 11\n",
    "correct_tokens_list = [correct_tokens[:11], correct_tokens[11:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 52])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_prompts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old optimization loop - unbatched\n",
    "\n",
    "# Initialize optimizer\n",
    "optim = torch.optim.Adam(mtf.masks.parameters(), lr=1e-2)\n",
    "\n",
    "alpha = 1e-1\n",
    "n_steps = 400\n",
    "p_norm = 1\n",
    "\n",
    "metric_history: list[dict[str]] = []\n",
    "pbar = tqdm(range(n_steps))\n",
    "\n",
    "for i in pbar:\n",
    "    # Forward pass through the model\n",
    "    # todo - get batched logits\n",
    "    logits = mtf.tl_model.forward(batch_prompts, return_type=\"logits\")  # shape [batch_size, seq_len, vocab_size]\n",
    "    logits = logits[:, -1, :]  # Taking logits corresponding to the last token for all prompts in the batch\n",
    "\n",
    "    # Compute target log probabilities\n",
    "    # Assume correct_token is a list containing the correct tokens for each prompt\n",
    "    target_log_prob = torch.log_softmax(logits, dim=-1)[range(len(correct_tokens)), correct_tokens]  # shape [batch_size]\n",
    "\n",
    "    # todo - compute average target log prob over all prompts\n",
    "    avg_target_log_prob = target_log_prob.mean()\n",
    "\n",
    "    # Regularization term calculations remain the same\n",
    "    mask_p_norm = sum(\n",
    "        (mask**p_norm).sum() for mask in mtf.masks.values()\n",
    "    ) ** (1 / p_norm)\n",
    "    mask_nonzero = sum((mask > 0).sum() for mask in mtf.masks.values())\n",
    "    mask_numel = sum(mask.numel() for mask in mtf.masks.values())\n",
    "    density = mask_nonzero / mask_numel\n",
    "    scaled_p_norm = alpha * mask_p_norm\n",
    "\n",
    "    # todo - use average target log prob over all prompts instead of just one\n",
    "    loss = -avg_target_log_prob + scaled_p_norm\n",
    "\n",
    "    # Gradient update step\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Clamp the masks to be between 0 and 1\n",
    "    with torch.no_grad():\n",
    "        for mask in mtf.masks.values():\n",
    "            mask.clamp_(0, 1)\n",
    "\n",
    "    # Logging\n",
    "    metrics = dict(\n",
    "        step=i,\n",
    "        loss=loss.item(),\n",
    "        avg_target_prob=torch.exp(avg_target_log_prob).item(),\n",
    "        target_prob=torch.exp(avg_target_log_prob).item(),\n",
    "        scaled_p_norm=scaled_p_norm.item(),\n",
    "        mask_p_norm=mask_p_norm.item(),\n",
    "        mask_nonzero=mask_nonzero.item(),\n",
    "        mask_numel=mask_numel,\n",
    "        density=density.item(),\n",
    "    )\n",
    "    metric_history.append(metrics)\n",
    "    pbar.set_postfix(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optim = torch.optim.Adam(mtf.masks.parameters(), lr=1e-2)\n",
    "\n",
    "alpha = 1e-1\n",
    "n_steps = 400\n",
    "p_norm = 1\n",
    "\n",
    "metric_history: list[dict[str]] = []\n",
    "pbar = tqdm(range(n_steps))\n",
    "\n",
    "for i in pbar:\n",
    "    # Forward pass through the model\n",
    "    # todo - get batched logits\n",
    "    for batch_prompts, correct_tokens in batched_prompts, correct_tokens_list:\n",
    "        logits = mtf.tl_model.forward(batch_prompts, return_type=\"logits\")  # shape [batch_size, seq_len, vocab_size]\n",
    "        logits = logits[:, -1, :]  # Taking logits corresponding to the last token for all prompts in the batch\n",
    "\n",
    "        # Compute target log probabilities\n",
    "        # Assume correct_token is a list containing the correct tokens for each prompt\n",
    "        target_log_prob = torch.log_softmax(logits, dim=-1)[range(len(correct_tokens)), correct_tokens]  # shape [batch_size]\n",
    "\n",
    "        # todo - compute average target log prob over all prompts\n",
    "        avg_target_log_prob = target_log_prob.mean()\n",
    "\n",
    "        # Regularization term calculations remain the same\n",
    "        mask_p_norm = sum(\n",
    "            (mask**p_norm).sum() for mask in mtf.masks.values()\n",
    "        ) ** (1 / p_norm)\n",
    "        mask_nonzero = sum((mask > 0).sum() for mask in mtf.masks.values())\n",
    "        mask_numel = sum(mask.numel() for mask in mtf.masks.values())\n",
    "        density = mask_nonzero / mask_numel\n",
    "        scaled_p_norm = alpha * mask_p_norm\n",
    "\n",
    "        # todo - use average target log prob over all prompts instead of just one\n",
    "        loss = -avg_target_log_prob + scaled_p_norm\n",
    "\n",
    "        # Gradient update step\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Clamp the masks to be between 0 and 1\n",
    "        with torch.no_grad():\n",
    "            for mask in mtf.masks.values():\n",
    "                mask.clamp_(0, 1)\n",
    "\n",
    "        # Logging\n",
    "        metrics = dict(\n",
    "            step=i,\n",
    "            loss=loss.item(),\n",
    "            avg_target_prob=torch.exp(avg_target_log_prob).item(),\n",
    "            target_prob=torch.exp(avg_target_log_prob).item(),\n",
    "            scaled_p_norm=scaled_p_norm.item(),\n",
    "            mask_p_norm=mask_p_norm.item(),\n",
    "            mask_nonzero=mask_nonzero.item(),\n",
    "            mask_numel=mask_numel,\n",
    "            density=density.item(),\n",
    "        )\n",
    "        metric_history.append(metrics)\n",
    "        pbar.set_postfix(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976b22d5818b4c388280ca81c6eb534c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 47.54 GiB total capacity; 46.46 GiB already allocated; 43.12 MiB free; 47.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# todo - get batched logits\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_prompts, correct_tokens \u001b[39min\u001b[39;00m batched_prompts, correct_tokens_list:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         logits \u001b[39m=\u001b[39m mtf\u001b[39m.\u001b[39;49mtl_model\u001b[39m.\u001b[39;49mforward(batch_prompts, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# shape [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         logits \u001b[39m=\u001b[39m logits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]  \u001b[39m# Taking logits corresponding to the last token for all prompts in the batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m# Compute target log probabilities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brunpod-pii/nas/pii-data/kaivu/prompt-injection-interp/notebooks/subnet-flow-proto.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m# Assume correct_token is a list containing the correct tokens for each prompt\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:365\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    362\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    363\u001b[0m         )\n\u001b[0;32m--> 365\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    366\u001b[0m         residual,\n\u001b[1;32m    367\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    368\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    369\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    370\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    371\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/transformer_lens/components.py:964\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m add_head_dimension(shortformer_pos_embed)\n\u001b[1;32m    960\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_attn_out(\n\u001b[1;32m    961\u001b[0m     \u001b[39m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[39m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[39m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    965\u001b[0m         query_input\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(query_input)\n\u001b[1;32m    966\u001b[0m         \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    967\u001b[0m         key_input\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(key_input)\n\u001b[1;32m    968\u001b[0m         \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    969\u001b[0m         value_input\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(value_input),\n\u001b[1;32m    970\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache_entry,\n\u001b[1;32m    971\u001b[0m     )\n\u001b[1;32m    972\u001b[0m )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mattn_only \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    974\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[1;32m    975\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m    976\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/transformer_lens/components.py:514\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask)\u001b[0m\n\u001b[1;32m    502\u001b[0m     qkv_einops_string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_q(\n\u001b[1;32m    505\u001b[0m     einsum(\n\u001b[1;32m    506\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_Q\n\u001b[1;32m    512\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    513\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_k(\n\u001b[0;32m--> 514\u001b[0m     einsum(\n\u001b[1;32m    515\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mqkv_einops_string\u001b[39m}\u001b[39;49;00m\u001b[39m, head_index d_model d_head \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m    516\u001b[0m \u001b[39m        -> batch pos head_index d_head\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    517\u001b[0m         key_input,\n\u001b[1;32m    518\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_K,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_K\n\u001b[1;32m    521\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    522\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_v(\n\u001b[1;32m    523\u001b[0m     einsum(\n\u001b[1;32m    524\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_V\n\u001b[1;32m    530\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m past_kv_cache_entry \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    533\u001b[0m     \u001b[39m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[39m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49meinsum(new_equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, equation, \u001b[39m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m~/.micromamba/envs/pii/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 47.54 GiB total capacity; 46.46 GiB already allocated; 43.12 MiB free; 47.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "optim = torch.optim.Adam(mtf.masks.parameters(), lr=1e-2)\n",
    "\n",
    "alpha = 1e-1\n",
    "n_steps = 400\n",
    "p_norm = 1\n",
    "\n",
    "metric_history: list[dict[str]] = []\n",
    "pbar = tqdm(range(n_steps))\n",
    "\n",
    "for i in pbar:\n",
    "    # Forward pass through the model\n",
    "    # todo - get batched logits\n",
    "    for batch_prompts, correct_tokens in batched_prompts, correct_tokens_list:\n",
    "        logits = mtf.tl_model.forward(batch_prompts, return_type=\"logits\")  # shape [batch_size, seq_len, vocab_size]\n",
    "        logits = logits[:, -1, :]  # Taking logits corresponding to the last token for all prompts in the batch\n",
    "\n",
    "        # Compute target log probabilities\n",
    "        # Assume correct_token is a list containing the correct tokens for each prompt\n",
    "        target_log_prob = torch.log_softmax(logits, dim=-1)[range(len(correct_tokens)), correct_tokens]  # shape [batch_size]\n",
    "\n",
    "        # todo - compute average target log prob over all prompts\n",
    "        avg_target_log_prob = target_log_prob.mean()\n",
    "\n",
    "        # Regularization term calculations remain the same\n",
    "        mask_p_norm = sum(\n",
    "            (mask**p_norm).sum() for mask in mtf.masks.values()\n",
    "        ) ** (1 / p_norm)\n",
    "        mask_nonzero = sum((mask > 0).sum() for mask in mtf.masks.values())\n",
    "        mask_numel = sum(mask.numel() for mask in mtf.masks.values())\n",
    "        density = mask_nonzero / mask_numel\n",
    "        scaled_p_norm = alpha * mask_p_norm\n",
    "\n",
    "        # todo - use average target log prob over all prompts instead of just one\n",
    "        loss = -avg_target_log_prob + scaled_p_norm\n",
    "\n",
    "        # Gradient update step\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Clamp the masks to be between 0 and 1\n",
    "        with torch.no_grad():\n",
    "            for mask in mtf.masks.values():\n",
    "                mask.clamp_(0, 1)\n",
    "\n",
    "        # Logging\n",
    "        metrics = dict(\n",
    "            step=i,\n",
    "            loss=loss.item(),\n",
    "            avg_target_prob=torch.exp(avg_target_log_prob).item(),\n",
    "            target_prob=torch.exp(avg_target_log_prob).item(),\n",
    "            scaled_p_norm=scaled_p_norm.item(),\n",
    "            mask_p_norm=mask_p_norm.item(),\n",
    "            mask_nonzero=mask_nonzero.item(),\n",
    "            mask_numel=mask_numel,\n",
    "            density=density.item(),\n",
    "        )\n",
    "        metric_history.append(metrics)\n",
    "        pbar.set_postfix(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtf.zero_threshold = 0\n",
    "mtf.one_threshold = 1.1\n",
    "plot_metrics(mtf, prompt, metric_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize remaining flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = np.stack(\n",
    "    [mtf.masks[f\"{i}_attn_out\"].detach().cpu().numpy() for i in range(32)]\n",
    ")\n",
    "attn_z_mask = np.stack(\n",
    "    [mtf.masks[f\"{i}_attn_z\"].detach().cpu().numpy() for i in range(32)], axis=0\n",
    ")\n",
    "attn_pattern_mask = np.stack(\n",
    "    [mtf.masks[f\"{i}_attn_pattern\"].detach().cpu().numpy() for i in range(32)],\n",
    ")\n",
    "mlp_mask = np.stack(\n",
    "    [mtf.masks[f\"{i}_mlp_out\"].detach().cpu().numpy() for i in range(32)]\n",
    ")\n",
    "\n",
    "attn_z_mask *= (attn_mask > 0)[:, :, np.newaxis]\n",
    "attn_mask.shape, attn_z_mask.shape, attn_pattern_mask.shape, mlp_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of attention mask\n",
    "subtokens = mtf.tl_model.to_tokens(prompt)[0, mtf.mask_start_idx :]\n",
    "tokenized_subprompt = \"|\".join(mtf.tl_model.to_string(x) for x in subtokens)\n",
    "print(tokenized_subprompt)\n",
    "\n",
    "for layer_idx in range(32):\n",
    "    for token_idx in range(mtf.n_tokens):\n",
    "        for head_idx in range(mtf.tl_model.cfg.n_heads):\n",
    "            weight = attn_z_mask[layer_idx, token_idx, head_idx]\n",
    "            if weight > 0:\n",
    "                print(\n",
    "                    f\"Layer {layer_idx},\"\n",
    "                    f\" head {head_idx},\"\n",
    "                    f\" token {token_idx}:\"\n",
    "                    f\" {weight=}\"\n",
    "                )\n",
    "\n",
    "for layer_idx in range(32):\n",
    "    for head_idx in range(mtf.tl_model.cfg.n_heads):\n",
    "        for token_idx_q in range(mtf.n_tokens):\n",
    "            for token_idx_k in range(mtf.n_tokens):\n",
    "                weight = attn_pattern_mask[\n",
    "                    layer_idx, head_idx, token_idx_q, token_idx_k\n",
    "                ]\n",
    "                if weight > 0:\n",
    "                    q_token = mtf.tl_model.to_string(subtokens[token_idx_q])\n",
    "                    k_token = mtf.tl_model.to_string(subtokens[token_idx_k])\n",
    "                    print(\n",
    "                        f\"Layer {layer_idx},\"\n",
    "                        f\" head {head_idx},\"\n",
    "                        f\" token_q {token_idx_q} ({q_token})\"\n",
    "                        f\" token_k {token_idx_k} ({k_token}):\"\n",
    "                        f\" {weight=}\"\n",
    "                    )\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "# plt.suptitle(tokenized_subprompt)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(attn_mask, origin=\"lower\")\n",
    "plt.xlabel(\"Token\")\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.colorbar()\n",
    "plt.title(\"attn_out mask\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(attn_z_mask.sum(axis=-1), origin=\"lower\")\n",
    "plt.xlabel(\"Token\")\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.colorbar()\n",
    "plt.title(\"attn_z mask (summed over heads)\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mlp_mask, origin=\"lower\")\n",
    "plt.xlabel(\"Token\")\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.colorbar()\n",
    "plt.title(\"mlp_out mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mask_values = (\n",
    "    torch.cat([mask.flatten() for mask in mtf.masks.values()])\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "\n",
    "thresholds = np.unique(\n",
    "    np.sort(np.concatenate([all_mask_values, np.array([0, 1])]))\n",
    ")\n",
    "thresh_metrics = []\n",
    "for thresh in tqdm(thresholds):\n",
    "    mtf.zero_threshold = thresh\n",
    "    mtf.one_threshold = 1\n",
    "    with torch.no_grad():\n",
    "        target_prob_0 = mtf.tl_model.forward(\n",
    "            prompt_tokens, return_type=\"logits\"\n",
    "        )[0, -1].softmax(dim=0)[correct_token]\n",
    "\n",
    "    mtf.zero_threshold = 0\n",
    "    mtf.one_threshold = max(thresh, 1e-9)\n",
    "    with torch.no_grad():\n",
    "        target_prob_1 = mtf.tl_model.forward(\n",
    "            prompt_tokens, return_type=\"logits\"\n",
    "        )[0, -1].softmax(dim=0)[correct_token]\n",
    "\n",
    "    thresh_metrics.append(\n",
    "        dict(\n",
    "            threshold=thresh,\n",
    "            target_prob_0=target_prob_0.item(),\n",
    "            target_prob_1=target_prob_1.item(),\n",
    "            n_nonzero=(all_mask_values > thresh).sum(),\n",
    "            n_ones=(all_mask_values > thresh).sum(),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresh = pd.DataFrame(thresh_metrics)\n",
    "\n",
    "print(\"# nonzero mask values:\", (all_mask_values > 0).sum())\n",
    "print(\"# total mask values:\", all_mask_values.size)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(\n",
    "    all_mask_values[all_mask_values > 0],\n",
    "    cumulative=True,\n",
    "    bins=100,\n",
    "    density=True,\n",
    ")\n",
    "plt.title(\"CDF of Mask Values\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(df_thresh.threshold, df_thresh.target_prob_0)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"0 Threshold\")\n",
    "plt.ylabel(\"Target Prob\")\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(df_thresh.n_nonzero, df_thresh.target_prob_0)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"# non-zero mask values\")\n",
    "plt.ylabel(\"Target Prob\")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(df_thresh.threshold, df_thresh.target_prob_1)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Target Prob\")\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot(df_thresh.n_ones, df_thresh.target_prob_1)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"# ones\")\n",
    "plt.ylabel(\"Target Prob\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
