{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate how the enrichment process of factual recall is impacted by competing circuits. We leverage [attribute lenses](https://arxiv.org/abs/2308.09124) to perform our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformer_lens.utils as tl_utils\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from pii import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b0520a6d96457d8f7a79a8bdba7444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You will need to login to huggingface first:\n",
    "#   git config --global credential.helper store\n",
    "#   huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a60cd701ab40b9949a6d4028447c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin. Berlin is the largest city in Germany and is known for its rich history, cultural attractions'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformerlens model\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    hf_model=hf_model,\n",
    "    device=\"cpu\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer,\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tl_model.generate(\n",
    "    \"The capital of Germany is\",\n",
    "    max_new_tokens=20,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(\n",
    "    person: str,\n",
    "    question: str,\n",
    "    correct: bool = True,\n",
    ") -> str:\n",
    "    return f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are an obedient and well-informed assistant who only ever responds with a single word.\n",
    "<</SYS>>\n",
    "\n",
    "{'' if correct else 'Give an incorrect answer to the question. '}\\\n",
    "The question is about {person}. {question} [/INST]\n",
    "\"\"\"\n",
    "#     return f\"\"\"\\\n",
    "# [INST] <<SYS>>\n",
    "# You only ever responds with a single word. \\\n",
    "# You always give the {'right' if correct else 'wrong'} answer.\n",
    "# <</SYS>>\n",
    "\n",
    "# I have a question about {person}. {question} [/INST]\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0. Logit: 23.21 Prob: 98.94% Tokens: |V|i|olin|</s>|\n",
      "Rank 1. Logit: 18.19 Prob:  0.65% Tokens: |\"|V|i|olin|\"|</s>|\n",
      "Rank 2. Logit: 17.15 Prob:  0.23% Tokens: |Vi|olin|</s>|\n",
      "Rank 3. Logit: 16.40 Prob:  0.11% Tokens: |VI|OL|IN|</s>|\n",
      "Rank 4. Logit: 15.02 Prob:  0.03% Tokens: |vi|olin|</s>|\n",
      "\n",
      "Rank 0. Logit: 16.52 Prob: 51.93% Tokens: |V|i|olin|</s>|\n",
      "Rank 1. Logit: 14.88 Prob: 10.14% Tokens: |B|ass|</s>|\n",
      "Rank 2. Logit: 14.62 Prob:  7.80% Tokens: |G|uit|ar|</s>|\n",
      "Rank 3. Logit: 14.55 Prob:  7.28% Tokens: |W|rong|!|It|zh|ak|Perl|man|is|a|viol|in|ist|.|</s>|\n",
      "Rank 4. Logit: 13.91 Prob:  3.81% Tokens: |D|rum|s|</s>|\n"
     ]
    }
   ],
   "source": [
    "# person = \"Valentina Lisitsa\" # Wrong\n",
    "person = \"Itzhak Perlman\"\n",
    "question = \"What instrument does this person play?\"\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question),\n",
    "    model=tl_model,\n",
    ")\n",
    "print()\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question, correct=False),\n",
    "    model=tl_model,\n",
    "    n_continuation_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m person \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMiles Davis\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat instrument does this person play?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m utils\u001b[39m.\u001b[39mget_top_responses(\n\u001b[1;32m      4\u001b[0m     prompt\u001b[39m=\u001b[39mgen_prompt(person, question),\n\u001b[1;32m      5\u001b[0m     model\u001b[39m=\u001b[39mtl_model,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m      8\u001b[0m utils\u001b[39m.\u001b[39mget_top_responses(\n\u001b[1;32m      9\u001b[0m     prompt\u001b[39m=\u001b[39mgen_prompt(person, question, correct\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m     model\u001b[39m=\u001b[39mtl_model,\n\u001b[1;32m     11\u001b[0m     n_continuation_tokens\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "person = \"Miles Davis\"\n",
    "question = \"What instrument does this person play?\"\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question),\n",
    "    model=tl_model,\n",
    ")\n",
    "print()\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question, correct=False),\n",
    "    model=tl_model,\n",
    "    n_continuation_tokens=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0. Logit: 21.03 Prob: 94.74% Tokens: |1|9|8|8|</s>|\n",
      "Rank 1. Logit: 17.51 Prob:  2.81% Tokens: |\"|1|9|8|8|\"|\n",
      "Rank 2. Logit: 16.88 Prob:  1.49% Tokens: |B|orn|:||1|9|\n",
      "Rank 3. Logit: 15.94 Prob:  0.58% Tokens: |D|ur|ant|</s>|\n",
      "Rank 4. Logit: 14.39 Prob:  0.12% Tokens: |Ke|vin|</s>|\n",
      "\n",
      "Rank 0. Logit: 17.65 Prob: 77.25% Tokens: |1|9|8|5|</s>|\n",
      "Rank 1. Logit: 15.51 Prob:  9.06% Tokens: |*||1|9|8|5|</s>|\n",
      "Rank 2. Logit: 14.56 Prob:  3.53% Tokens: |W|rong|.|</s>|\n",
      "Rank 3. Logit: 13.84 Prob:  1.72% Tokens: |�|�|�|�|</s>|\n",
      "Rank 4. Logit: 13.43 Prob:  1.14% Tokens: |U|h|...||1|9|8|5|.|</s>|\n"
     ]
    }
   ],
   "source": [
    "person = \"Kevin Durant\"\n",
    "question = \"What year was this person born in?\"\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question),\n",
    "    model=tl_model,\n",
    ")\n",
    "print()\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question, correct=False),\n",
    "    model=tl_model,\n",
    "    n_continuation_tokens=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0. Logit: 24.01 Prob: 98.65% Tokens: |M|ale|</s>|\n",
      "Rank 1. Logit: 18.66 Prob:  0.47% Tokens: |Male|</s>|\n",
      "Rank 2. Logit: 18.30 Prob:  0.33% Tokens: |Man|</s>|\n",
      "Rank 3. Logit: 18.14 Prob:  0.28% Tokens: |male|</s>|\n",
      "Rank 4. Logit: 17.23 Prob:  0.11% Tokens: |\"|M|ale|\"|</s>|\n",
      "\n",
      "Rank 0. Logit: 18.00 Prob: 62.80% Tokens: |F|em|ale|</s>|\n",
      "Rank 1. Logit: 17.17 Prob: 27.41% Tokens: |W|oman|</s>|\n",
      "Rank 2. Logit: 15.22 Prob:  3.87% Tokens: |M|ale|</s>|\n",
      "Rank 3. Logit: 14.00 Prob:  1.15% Tokens: |She|</s>|\n",
      "Rank 4. Logit: 13.92 Prob:  1.06% Tokens: |Fem|ale|</s>|\n"
     ]
    }
   ],
   "source": [
    "person = \"Kevin Durant\"\n",
    "question = \"What gender is this person?\"\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question),\n",
    "    model=tl_model,\n",
    ")\n",
    "print()\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question, correct=False),\n",
    "    model=tl_model,\n",
    "    n_continuation_tokens=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0. Logit: 20.54 Prob: 55.06% Tokens: |Th|under|</s>|\n",
      "Rank 1. Logit: 19.98 Prob: 31.21% Tokens: |W|ar|riors|</s>|\n",
      "Rank 2. Logit: 18.30 Prob:  5.83% Tokens: |N|ets|</s>|\n",
      "Rank 3. Logit: 17.90 Prob:  3.91% Tokens: |Ok|ay|!|Kevin|Durant|plays|\n",
      "Rank 4. Logit: 16.84 Prob:  1.36% Tokens: |Bro|ok|lyn|</s>|\n",
      "\n",
      "Rank 0. Logit: 17.13 Prob: 69.09% Tokens: |B|ull|s|</s>|\n",
      "Rank 1. Logit: 14.81 Prob:  6.81% Tokens: |W|ar|riors|</s>|\n",
      "Rank 2. Logit: 14.20 Prob:  3.69% Tokens: |*|Brook|lyn|</s>|\n",
      "Rank 3. Logit: 14.19 Prob:  3.66% Tokens: |N|ets|</s>|\n",
      "Rank 4. Logit: 13.87 Prob:  2.66% Tokens: |M|iami|</s>|\n"
     ]
    }
   ],
   "source": [
    "person = \"Kevin Durant\"\n",
    "question = \"What team does this person play for?\"\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question),\n",
    "    model=tl_model,\n",
    ")\n",
    "print()\n",
    "utils.get_top_responses(\n",
    "    prompt=gen_prompt(person, question, correct=False),\n",
    "    model=tl_model,\n",
    "    n_continuation_tokens=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up attribute lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
